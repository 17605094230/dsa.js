image:extracted-media/media/image1.png[image,width=811,height=1080]

*Illustrated*

*Data Structures*

*&*

*Algorithms*

1st Edition

*Adrian Mejia*

ILLUSTRATED DATA STRUCTURES & ALGORITHMS, FIRST EDITION

Copyright © 2018 Adrian Mejia

All rights reserved.

For online information and ordering this and other books, please visit
https://adrianmejia.com. The publisher offers discounts on this book
when ordered in quantity for more information contact
sales@adrianmejia.com.

No part of this publication maybe produced, the store in the retrieval
system, or transmitted, in any form or by mean electronic, mechanical,
photocopying, or otherwise, without prior written permission of the
publisher.

While every precaution has been taking in the preparation of this book,
the publisher and author assume no responsibility for errors or
omissions, or for damages resulting from the use of the information
contained herein.

First edition, August 2018.

Production reference: 1536112322283

[[_Toc525822190]]Table of Contents

link:#_Toc525822190[Table of Contents 3]

link:#_Toc525822191[Detailed Table of Contents 5]

link:#_Toc525822192[Learning Algorithms Analysis 12]

link:#comparing-algorithms[Comparing Algorithms 12]

link:#increasing-your-code-performance[Increasing your code performance
13]

link:#summary[Summary 17]

link:#_Toc525822196[Eight Running Times Every Developer Should Know 18]

link:#constant-o1[Constant O(1) 19]

link:#logarithmic-olog-n[Logarithmic O(log n) 20]

link:#linear-on[Linear O(n) 21]

link:#linearithmic-on-log-n[Linearithmic O(n log n) 23]

link:#quadratic-on2[Quadratic O(n^2^) 25]

link:#cubic-on3-and-polynomial-onc[Cubic O(n^3^) and Polynomial O(n^c^)
26]

link:#exponential-o2n[Exponential O(2^n^) 28]

link:#factorial-on[Factorial O(n!) 29]

link:#summary-1[Summary 30]

link:#_Toc525822206[Mastering Linear Data Structures Ins and Outs 32]

link:#array[Array 32]

link:#linked-list[Linked List 37]

link:#stack[Stack 49]

link:#queue[Queue 52]

link:#summary-2[Summary 53]

link:#_Toc525822212[Demystifying Non-Linear Data Structures 55]

link:#tree-data-structure[Tree Data Structure 55]

link:#map[Map 67]

link:#set[Set 78]

link:#graph-data-structure[Graph Data Structure 84]

link:#summary-3[Summary 92]

link:#_Toc525822218[Learning Fast Sorting Algorithms 93]

link:#avoiding-slow-sorting-algorithms[Avoiding Slow Sorting Algorithms
93]

link:#understanding-efficient-sorting-algorithms[Understanding Efficient
Sorting Algorithms 93]

link:#summary-4[Summary 94]

link:#_Toc525822222[Searching Efficiently 95]

link:#linear-search[Linear Search 95]

link:#searching-in-a-graph[Searching in a Graph 95]

link:#shortest-path-with-dijkstra[Shortest Path with Dijkstra 96]

link:#summary-5[Summary 96]

link:#_Toc525822227[Balancing Binary Search Trees for Max Performance
97]

link:#tree-rotations[Tree Rotations 97]

link:#avl-tree[AVL Tree 98]

link:#summary-6[Summary 98]

link:#_Toc525822231[Algorithmic Thinking 99]

link:#algorithmic-paradigms[Algorithmic Paradigms 99]

link:#topic[Topic 100]

link:#topic-1[Topic 100]

link:#summary-7[Summary 101]

link:#_Toc525822236[Stepping up your game with Advanced Data Structures
102]

link:#heap[Heap 102]

link:#tries[Tries 103]

[[_Toc525822191]]Detailed Table of Contents

link:#_Toc525822190[Table of Contents 3]

link:#_Toc525822191[Detailed Table of Contents 5]

link:#_Toc525822192[Learning Algorithms Analysis 12]

link:#comparing-algorithms[Comparing Algorithms 12]

link:#increasing-your-code-performance[Increasing your code performance
13]

link:#calculating-space-and-time-complexity[Calculating Space and Time
Complexity 14]

link:#simplifying-complexity-with-asymptotic-analysis[Simplifying
Complexity with Asymptotic Analysis 15]

link:#learning-what-big-o-notation-is-all-about[Learning what Big-O
Notation is all about 15]

link:#summary[Summary 17]

link:#_Toc525822196[Eight Running Times Every Developer Should Know 18]

link:#constant-o1[Constant O(1) 19]

link:#finding-if-an-array-is-empty[Finding if an array is empty 20]

link:#logarithmic-olog-n[Logarithmic O(log n) 20]

link:#searching-on-a-sorted-array[Searching on a sorted array 20]

link:#linear-on[Linear O(n) 21]

link:#finding-duplicates-in-an-array-using-a-map[Finding duplicates in
an array using a map 22]

link:#linearithmic-on-log-n[Linearithmic O(n log n) 23]

link:#sorting-elements-in-an-array[Sorting elements in an array 23]

link:#quadratic-on2[Quadratic O(n^2^) 25]

link:#finding-duplicates-in-an-array-naïve-approach[Finding duplicates
in an array (naïve approach) 26]

link:#cubic-on3-and-polynomial-onc[Cubic O(n^3^) and Polynomial O(n^c^)
26]

link:#programming-a-multi-variable-equation-solver[Programming a
multi-variable equation solver 27]

link:#exponential-o2n[Exponential O(2^n^) 28]

link:#finding-subsets-of-a-set[Finding subsets of a set 28]

link:#factorial-on[Factorial O(n!) 29]

link:#getting-all-permutations-of-a-word[Getting all permutations of a
word 29]

link:#summary-1[Summary 30]

link:#_Toc525822206[Mastering Linear Data Structures Ins and Outs 32]

link:#array[Array 32]

link:#insertion[Insertion 33]

link:#inserting-at-the-beginning-of-the-array[Inserting at the beginning
of the array 34]

link:#inserting-at-the-middle-of-the-array[Inserting at the middle of
the array 34]

link:#inserting-at-the-end-of-the-array[Inserting at the end of the
array 34]

link:#searching-by-value-and-index[Searching by value and index 35]

link:#deletion[Deletion 36]

link:#deleting-element-from-the-beginning[Deleting element from the
beginning 36]

link:#deleting-element-from-the-middle[Deleting element from the middle
37]

link:#deleting-element-from-the-end[Deleting element from the end 37]

link:#array-complexity[Array Complexity 37]

link:#linked-list[Linked List 37]

link:#singly-linked-list[Singly Linked List 37]

link:#doubly-linked-list[Doubly Linked List 38]

link:#linked-list-vs-array[Linked List vs Array 39]

link:#insertion-1[Insertion 40]

link:#inserting-element-at-the-beginning-of-the-list[Inserting element
at the beginning of the list 40]

link:#inserting-element-at-the-end-of-the-list[Inserting element at the
end of the list 41]

link:#inserting-element-at-the-middle-of-the-list[Inserting element at
the middle of the list 42]

link:#searching-by-value[Searching by value 44]

link:#searching-by-index[Searching by index 45]

link:#deletion-1[Deletion 45]

link:#deleting-element-from-the-head[Deleting element from the head 45]

link:#deleting-element-from-the-tail[Deleting element from the tail 46]

link:#deleting-element-from-the-middle-1[Deleting element from the
middle 47]

link:#linked-list-complexity-vs-array-complexity[Linked List Complexity
vs Array Complexity 49]

link:#stack[Stack 49]

link:#insertion-2[Insertion 50]

link:#deletion-2[Deletion 51]

link:#implementation-usage[Implementation Usage 51]

link:#stack-complexity[Stack Complexity 51]

link:#queue[Queue 52]

link:#insertion-3[Insertion 52]

link:#deletion-3[Deletion 53]

link:#implementation-usage-1[Implementation usage 53]

link:#queue-complexity[Queue Complexity 53]

link:#summary-2[Summary 53]

link:#_Toc525822212[Demystifying Non-Linear Data Structures 55]

link:#tree-data-structure[Tree Data Structure 55]

link:#implementing-a-tree[Implementing a Tree 56]

link:#basic-concepts[Basic concepts 57]

link:#types-of-binary-trees[Types of Binary Trees 57]

link:#binary-tree[Binary Tree 57]

link:#binary-search-tree-bst[Binary Search Tree (BST) 58]

link:#binary-heap[Binary Heap 58]

link:#implementing-a-binary-search-tree[Implementing a Binary Search
Tree 60]

link:#inserting-new-elements-in-a-bst[Inserting new elements in a BST
60]

link:#finding-a-value-in-a-bst[Finding a value in a BST 62]

link:#removing-elements-from-a-bst[Removing elements from a BST 63]

link:#differentiating-a-balanced-and-non-balanced-tree[Differentiating a
balanced and non-balanced Tree 66]

link:#tree-complexity[Tree Complexity 67]

link:#map[Map 67]

link:#hashmap-vs-treemap[HashMap vs TreeMap 68]

link:#learning-how-hash-maps-work[Learning how hash maps work 69]

link:#designing-an-optimized-hash-function[Designing an optimized hash
function 70]

link:#implementing-a-hashmap-in-javascript[Implementing a HashMap in
JavaScript 73]

link:#inserting-elements-in-a-hashmap[Inserting elements in a HashMap
74]

link:#rehashing-the-hashmap[Rehashing the HashMap 75]

link:#getting-values-out-of-a-hashmap[Getting values out of a HashMap
75]

link:#deleting-from-a-hashmap[Deleting from a HashMap 76]

link:#hashmap-time-complexity[HashMap time complexity 76]

link:#implementing-a-treemap[Implementing a TreeMap 76]

link:#inserting-values-into-a-treemap[Inserting values into a TreeMap
77]

link:#getting-values-out-of-a-treemap[Getting values out of a TreeMap
77]

link:#deleting-values-from-a-treemap[Deleting values from a TreeMap 78]

link:#treemap-time-complexity-vs-hashmap[TreeMap Time complexity vs
HashMap 78]

link:#set[Set 78]

link:#hashset-vs-treeset[HashSet vs TreeSet 79]

link:#implementing-a-treeset[Implementing a TreeSet 79]

link:#adding-elements-to-a-treeset[Adding elements to a TreeSet 80]

link:#searching-for-values-in-a-treeset[Searching for values in a
TreeSet 80]

link:#converting-treeset-to-array[Converting TreeSet to Array 81]

link:#deleting-elements-from-a-treeset[Deleting elements from a TreeSet
81]

link:#implementing-a-hashset[Implementing a HashSet 82]

link:#inserting-values-to-a-hashset[Inserting values to a HashSet 82]

link:#finding-values-in-a-hashset[Finding values in a HashSet 83]

link:#deleting-values-from-a-hashset[Deleting values from a HashSet 83]

link:#hashset-vs-hashmap-time-complexity[HashSet vs HashMap Time
Complexity 83]

link:#graph-data-structure[Graph Data Structure 84]

link:#graph-properties[Graph Properties 84]

link:#directed-graph-vs-undirected[Directed Graph vs Undirected 85]

link:#graph-cycles[Graph Cycles 86]

link:#connected-vs-disconnected-vs-complete-graphs[Connected vs
Disconnected vs Complete Graphs 87]

link:#weighted-graphs[Weighted Graphs 87]

link:#exciting-graph-applications-in-real-world[Exciting Graph
applications in real-world 88]

link:#representing-graphs[Representing Graphs 89]

link:#adjacency-matrix[Adjacency Matrix 89]

link:#adjacency-list[Adjacency List 90]

link:#adding-a-vertex[Adding a vertex 91]

link:#adding-an-edge[Adding an edge 91]

link:#querying-adjacency[Querying Adjacency 91]

link:#deleting-a-vertex[Deleting a vertex 91]

link:#deleting-an-edge[Deleting an edge 91]

link:#graph-complexity[Graph Complexity 91]

link:#summary-3[Summary 92]

link:#_Toc525822218[Learning Fast Sorting Algorithms 93]

link:#avoiding-slow-sorting-algorithms[Avoiding Slow Sorting Algorithms
93]

link:#selection-sort[Selection Sort 93]

link:#bubble-sort[Bubble Sort 93]

link:#insertion-sort[Insertion Sort 93]

link:#understanding-efficient-sorting-algorithms[Understanding Efficient
Sorting Algorithms 93]

link:#merge-sort[Merge Sort 94]

link:#quick-sort[Quick Sort 94]

link:#tim-sort[Tim Sort 94]

link:#heapsort[Heapsort 94]

link:#radix-sort[Radix Sort 94]

link:#summary-4[Summary 94]

link:#_Toc525822222[Searching Efficiently 95]

link:#linear-search[Linear Search 95]

link:#linear-search-1[Linear Search 95]

link:#binary-search[Binary Search 95]

link:#sub-topic[Sub-topic 95]

link:#searching-in-a-graph[Searching in a Graph 95]

link:#depth-first-search-dfs[Depth First Search (DFS) 95]

link:#breadth-first-search-bfs[Breadth First Search (BFS) 96]

link:#sub-topic-1[Sub-topic 96]

link:#shortest-path-with-dijkstra[Shortest Path with Dijkstra 96]

link:#sub-topic-2[Sub-topic 96]

link:#sub-topic-3[Sub-topic 96]

link:#sub-topic-4[Sub-topic 96]

link:#summary-5[Summary 96]

link:#_Toc525822227[Balancing Binary Search Trees for Max Performance
97]

link:#tree-rotations[Tree Rotations 97]

link:#left-rotation[Left Rotation 97]

link:#right-rotation[Right Rotation 97]

link:#left-right-rotation[Left-Right Rotation 97]

link:#right-left-rotation[Right-Left Rotation 97]

link:#avl-tree[AVL Tree 98]

link:#insertion-4[Insertion 98]

link:#search-by-value[Search by Value 98]

link:#deletion-4[Deletion 98]

link:#summary-6[Summary 98]

link:#_Toc525822231[Algorithmic Thinking 99]

link:#algorithmic-paradigms[Algorithmic Paradigms 99]

link:#brute-force[Brute Force 99]

link:#greedy[Greedy 99]

link:#divide-and-conquer[Divide and Conquer 100]

link:#dynamic-programming[Dynamic Programming 100]

link:#topic[Topic 100]

link:#sub-topic-5[Sub-topic 100]

link:#sub-topic-6[Sub-topic 100]

link:#sub-topic-7[Sub-topic 100]

link:#topic-1[Topic 100]

link:#sub-topic-8[Sub-topic 100]

link:#sub-topic-9[Sub-topic 101]

link:#sub-topic-10[Sub-topic 101]

link:#summary-7[Summary 101]

link:#_Toc525822236[Stepping up your game with Advanced Data Structures
102]

link:#heap[Heap 102]

link:#insert[Insert 102]

link:#heapify[Heapify 102]

link:#find-maxmin[Find max/min 102]

link:#extract-maxmin[Extract max/min 102]

link:#increase-key[Increase Key 103]

link:#delete[Delete 103]

link:#merge[Merge 103]

link:#tries[Tries 103]

link:#applications[Applications 104]

link:#insert-word[Insert word 104]

link:#suggesting-next-characters[Suggesting next characters 104]

link:#delete-word[Delete Word 104]

1

[[_Toc525822192]]Learning Algorithms Analysis

Chances are you are reading this book because you want to write better
and faster code. How can you do that? Can you time how long it takes to
run a program? Of course, you can! However, if you run the same program
on a smart watch, cellphone or desktop computer it will give you
different times.

image:extracted-media/media/image3.png[image,width=528,height=137]

Wouldn't it be great if we can compare algorithms regardless of the
hardware where we run them? That's what *time complexity* is for! But
why stop with the running time? We could also compare the memory “used”
by different algorithms and we called that *space complexity*.

In this chapter you will learn

* _____________________________________________________
What’s the best way to measure your code performance.
_____________________________________________________
* ______________________________________________________
Learn how to use Big O notation to compare algorithms.
______________________________________________________
* ______________________________________________________________
How to use algorithms analysis to improve your programs speed.
______________________________________________________________

= Comparing Algorithms

Before going deeper, into space and time complexity, let's define what
an algorithm is.

Algorithms (as you might know) are steps of how to do some task. When
you cook, you follow a recipe (or an algorithm) to prepare a dish. If
you play a game, you are devising strategies (or an algorithm) to help
you win. Likewise, algorithms in computers are a set of instructions
used to solve a problem.

Algorithms are instructions to perform a task

There are “good” algorithms and “bad” algorithms. The good ones are
fast; the bad ones are slow. Slow algorithms cost more money and make
some calculations impossible in our lifespan!

Just to give you a clearer picture how different algorithms perform as
the input size grows.

Table 1 - Relationship between algorithm input size and time taken to
complete

[cols=",,,,,",options="header",]
|=======================================================================
|Input size -> |10 |100 |10k |100k |1M
|Finding if a number is odd |< 1 sec. |< 1 sec. |< 1 sec. |< 1 sec. |< 1
sec.

|Sorting elements in array with merge sort |< 1 sec. |< 1 sec. |< 1 sec.
|few sec. |20 sec.

|Sorting elements in array with Bubble Sort |< 1 sec. |< 1 sec. |2
minutes |3 hours |12 days

|Finding all subsets of a given set |< 1 sec. |40,170 trillion years |>
centillion years |∞ |∞

|Find all permutations of a string |4 sec. |> vigintillion years |>
centillion years |∞ |∞
|=======================================================================

You can really notice the difference between a good algorithm and bad
with the sorting array elements examples: merge-sort vs bubble sort.
Organizing 1 million elements with merge sort takes 20 seconds while
bubble sort takes 12 days, ouch! The amazing thing is that both programs
are measured on the same hardware with exactly the same data!

After completing this book, you are going to *think differently*. You
will be able to scale your programs while you are designing them. Find
bottlenecks of existing software and have an "algorithmic toolbox" to
switch algorithms and make them faster without having to upgrade
hardware. 💸

= Increasing your code performance

The first step to improve your code performance is to measure it. As
somebody said:

Measurement is the first step that leads to control and eventually to
improvement. If you can’t measure something, you can’t understand it. If
you can’t understand it, you can’t control it. If you can’t control it,
you can’t improve it. - H. J. Harrington

In this section we are going to learn the basics to measuring our
current code performance and compare it with others.

== Calculating Space and Time Complexity

Time complexity, in computer science, is a function that describes the
amount of operations a program will execute given the size of the input
n. The same applies for space complexity but instead of the amount of
operations executed it will be the amount of memory used additional to
the input.

How do get a function that give us the amount of operations that will
executed? Well, we count line by line and mind code inside loops. For
instance, we have a function to find the minimum value on an array
called getMin:

Fix shadows on the following image

image:extracted-media/media/image4.png[image,width=528,height=269]

Figure 1 - Translating lines of code to approximate number of operations

Assuming that each line of code is an operation, we get the following
that the number of operations given the input size "n" is:

_3n+3_

That means that if give an array of 3 elements e.g. getMin([3, 2, 9]),
then it will execute around _3(3)+3 = 12_ operations. Of course, this is
not exact. Line 12 is only executed if the condition on line 11 is met.
As you might learn in the next section, we want to get the big picture
and get rid of smaller terms in order to compare algorithms easier.

Calculating the *space complexity* is very similar but, instead of
operations, we keep track of the “variables” and memory used. In the
getMin example, we just create a single variable called min. So, the
space complexity is 1. If we had to copy values to another array then
the space complexity would be n.

== Simplifying Complexity with Asymptotic Analysis

Asymptotic analysis is the of functions when their inputs approaches
infinity.

In the previous example we analyzed getMin with an array of size 3, what
happen size is 10 or 10k or a million?

Table 2 - Operations performed by an algorithm with a time complexity of
3n+3

[cols=",,",options="header",]
|===========================
|n (size) |Operations |total
|10 |3(10) + 3 |33
|10k |3(10k)+3 |30,003
|1M |3(1M)+3 |3,000,003
|===========================

As the input size n grows bigger and bigger then the expression _3n+3_
could be represented as _3n_ or even _n_. This might look like a stretch
at first, but you will see that what matters the most is the order of
the function rather than lesser terms and constants. Actually, there’s a
notation called *Big O*, where O refers to the *order of the function*.

If you have a program which run time is like

_7n3 + 3n2 + 5_

You can safely say that its run time is _n^3^_ since the others term
will become less and less significant as the inputs grows bigger.

== Learning what Big-O Notation is all about

Big O notation, only cares about the “biggest” terms in the time/space
complexity. So, it combines what we learn about time and space
complexity, asymptotic analysis and adds worst-case scenario.

All algorithms have 3 scenarios:

* Best-case scenario: the most favorable input where the program will
take the least amount of operations to complete. E.g. array already
sorted for a sorting algorithm.
* Average-case scenario: the most common the input comes. E.g. array
items in random order for a sorting algorithm.
* Worst-case scenario: the inputs are arranged in such a way that cause
the program to take the longest possible to complete the task. E.g.
array items in reversed order for a sorting algorithm.

To sum up:

Big O only cares about the highest order of the run time function and
the worst-case scenario.

There are many common notations like polynomial, _O(n^2^)_ like we saw
in the getMin example; constant O(1) and many more that we are going to
explore in the next chapter.

Again, time complexity is not a direct measure of how long a program
takes to execute but rather how many operations it executes in function
of the input. However, there’s a relationship between time and
operations executed. This changes from hardware to hardware but it gives
you an idea.

Readers might not know what this O(n!) means…

Table 3 - How long an algorithm takes to run based on their time
complexity and input size

[cols=",,,,,,",options="header",]
|===============================================================
|Input Size |O(1) |O(n) |O(n log n) |O(n^2^) |O(2^n^) |O(n!)
|1 |< 1 sec. |< 1 sec. |< 1 sec. |< 1 sec. |< 1 sec. |< 1 sec.
|10 |< 1 sec. |< 1 sec. |< 1 sec. |< 1 sec. |< 1 sec. |4 seconds
|10k |< 1 sec. |< 1 sec. |< 1 sec. |2 minutes |∞ |∞
|100k |< 1 sec. |< 1 sec. |1 second |3 hours |∞ |∞
|1M |< 1 sec. |1 second |20 seconds |12 days |∞ |∞
|===============================================================

This just an illustration since in a different hardware the times will
be slightly different. These times are under the assumption of running
on 1 GHz CPU and that it can execute on average one instruction in 1
nanosecond (usually takes more time). Also, bear in mind that each line
might be translated into dozens of CPU instructions depending on the
programming language. Regardless, bad algorithms still perform badly
even in a super computer.

= Summary

In this chapter we learned how you can measure you algorithm performance
using time complexity. Rather than timing how long you program take to
run you can approximate the number of operation it will perform based on
the input size.

We went thought the process of deducting the time complexity from a
simple algorithm. We learned about time and space complexity and how
they can be translated to Big O notation. Big O refers to the order of
the function.

In the next section, we are going to provide examples of each of the
most common time complexities!

2

[[_Toc525822196]]Eight Running Times Every Developer Should Know

There are many kinds of algorithms but most of them falls into one of
the time complexities that we are going to explore here:

* ___________________
Constant time: O(1)
___________________
* __________________________
Logarithmic time: O(log n)
__________________________
* _________________
Linear time: O(n)
_________________
* _____________________________
Linearithmic time: O(n log n)
_____________________________
* _______________________
Quadratic time: O(n^2^)
_______________________
* ___________________
Cubic time: O(n^3^)
___________________
* _________________________
Exponential time: O(2^n^)
_________________________
* _____________________
Factorial time: O(n!)
_____________________

We a going to provide examples for each one of them. Before we dive in,
here’s a plot with all of them.

image:extracted-media/media/image5.png[image,width=528,height=422]

Figure 2 - CPU Operations vs Input size

This graph shows how the algorithm running time affects the CPU work as
the input size grows. As you can see O(1) and O(log n) are very
scalable. However, O(n^2^) and worst can make your computer burn on
large data sets. 🔥 We are going to give some examples so you can
identify each one.

= Constant O(1)

Represented as *O(1)*, it means that regardless of the input size the
number of operations executed is always the same. Let’s see an example

== Finding if an array is empty

image:extracted-media/media/image6.png[image,width=528,height=401]As you
can see if thing is an array of 10 elements or an array of 10M elements
it would take the same amount of time to execute. It doesn’t get any
more performant than this!

= Logarithmic O(log n)

Represented in Big O notation as *O(log n)*, when an algorithms has this
running time it means that as the size of the input grows the number of
operations grows very slowly. This make this kind of algorithms very
scalable. One example is the *binary search*.

== Searching on a sorted array

The binary search only works for sorted lists. It starts searching for
an element on the middle and then moves to the right or left depending
if the value you are looking for is bigger or smaller.

image:extracted-media/media/image7.png[image,width=528,height=437]

This is a recursive algorithm, which means that the function
binarySearch calls itself multiple times until the solution is found.
The binary search split the array in half every time.

Finding the runtime of recursive algorithms is not very obvious
sometimes. It requires some intuition and following what the program is
doing. The binarySearch divides the input in half each time. As a rule
of thumb, when we have an algorithm that divides the input in half on
each call we can say that has a logarithmic runtime: O(log n).

= Linear O(n)

This is one of the most commons. It’s represented as *O(n)*. Usually an
algorithm has a linear running time when it iterates over all the
elements in the input.

== Finding duplicates in an array using a map

Let’s say that we want to find duplicate elements in an array. What’s
the first implementation that comes to mind? Check out this
implementation:

image:extracted-media/media/image8.png[image,width=528,height=383]

hasDuplicates has multiple scenarios:

* Best-case scenario: first two elements are duplicates. It only has to
visit two elements.
* Worst-case scenario: no duplicates or duplicates are the last two. In
either case it has to visit every element on the array.
* Average-case scenario: duplicates are somewhere in the middle of the
array. Only, half of the array has be visited.

As we learned before, the big O cares about the worst-case scenario,
where we would have to visit every element on the array. So, we have an
*O(n)* runtime.

Space complexity is also *O(n)* since we have a map that in the worst
case (no duplicates) it will hold every word.

= Linearithmic O(n log n)

An algorithm with a linearithmic runtime is represented as O(n log n).
This one is important because is the best runtime for sorting! Let’s see
the merge-sort.

== Sorting elements in an array

The merge sort, like its name indicates, has two functions merge and
sort. Let’s start with the sort function:

image:extracted-media/media/image9.png[image,width=528,height=383]

Starting with the sort part, we basically divide the array in 2 halves
and then merge them (line 16) recursively with the following function:

image:extracted-media/media/image10.png[image,width=528,height=380]

The merge function combines arrays in ascending order. Let’s say that we
want to sort the array [9, 2, 5, 1, 7, 6]. In the following illustration
you can see what each function does.

image:extracted-media/media/image11.png[image,width=316,height=389]

Figure 3 - Mergesort visualization. Shows the split, sort and merge
steps

How do we obtain the running time of the merge sort algorithm? The
mergesort divides the array in half each time in the split phase, log n,
and the merge function join each splits, n. The total work we have *O(n
log n)*. There more formal ways to reach to this runtime like using the
https://adrianmejia.com/blog/2018/04/24/analysis-of-recursive-algorithms/[Master
Method] and
https://www.cs.cornell.edu/courses/cs3110/2012sp/lectures/lec20-master/lec20.html[recursion
trees].

= Quadratic O(n^2^)

Running times that are quadratic, O(n^2^), are the ones to watch out
for. They usually don’t scale well when they have large data to process.

Usually, they have double nested loops that where each one visits all or
most elements in the input. One example of this is a naïve
implementation to find duplicate words on an array.

== Finding duplicates in an array (naïve approach)

If you remember we have solved this problem on the “Linear” section. We
solved this problem before a O(n), let’s analyze this time with a
O(n^2^):

image:extracted-media/media/image12.png[image,width=527,height=389]

As you can see, we have two nested loops causing the running time to be
quadratic. How much different is a linear vs quadratic algorithm?

Let’s say you want to find duplicated phone number in a phone directory
of a city (e.g. 1 million people). If you use this quadratic solution
you would have to wait for ~12 days to get an answer , while if you use
the linear solution you will get the answer in seconds!

= Cubic O(n^3^) and Polynomial O(n^c^)

Cubic *O(n^3^)* and higher polynomial functions usually involves many
nested loops. As an example of a cubic algorithm looks like let’s say
you want to solve a multi-variable equation (using brute force):

== Programming a multi-variable equation solver

Let’s say we want to find the solution for this multi-variable equation:

3x + 9y + 8z = 79

A naïve approach to solve this will be the following program:

image:extracted-media/media/image13.png[image,width=528,height=448]

Warning: This just an example, there are better ways to solve
multi-variable equations.

As you can see three nested loops usually translates to O(n^3^). If you
have a 4 variable equation and four nested loops it would be O(n^4^) and
so on. When we have a runtime in the form of _O(n^c^)_, where _c > 1_,
we can refer as a *polynomial runtime*.

= Exponential O(2^n^)

Exponential runtimes, O(2^n^), means that every time the input grows by
1 the amount of operations done by the algorithms doubles. Exponential
programs are only usable for very small size of inputs (<100) otherwise
it might not finish on your lifetime ️. Let’s do an example.

== Finding subsets of a set

Finding all distinct subsets of a given set.

image:extracted-media/media/image14.png[image,width=528,height=401]

The way this algorithm generates all subsets is:

1.  Base case is empty element (line 13). E.g. ['']
2.  For each element from the input append it to the results array (line
16)
3.  The new results array will be what it was before + the duplicated
with the appended element (line 17)

Every time the input grows by one the size the result array doubles.
That’s why it has an *O(2^n^)*.

= Factorial O(n!)

Factorial runtime, O(n!), is not scalable at all. Even with input sizes
of ~10 elements it will take a couple of seconds to compute. It’s that
slow!

One classic example of an O(n!) algorithm is finding all the different
words that can be formed with a given set of letters.

== Getting all permutations of a word

image:extracted-media/media/image15.png[image,width=528,height=377]

As you can see in getPermutations function, the resulting array is the
factorial of the word length. E.g.

getPermutations('a') _// => ['a'] _

_// 1! = 1_

getPermutations('ab') _// => ['ab', 'ba']_

_// 2! = 2_

getPermutations('mad')_//=> ['mad', 'mda', 'amd', 'adm', 'dma', 'dam']_

// 3! = 3x2x1 = 6

Factorial start very slow but it quickly become unmanageable.

Factorial

A factorial, is the multiplication of all the numbers less than itself
down to 1. E.g:

5! = 5x4x3x2x1 = 120

10! = 3,628,800

As you can see this is not very scalable. A word size of just 11
characters would take a couple of hours in most computers!

= Summary

We went through 8 of the most common time complexities and provided
examples for each of them. Hopefully, this will give you a toolbox to
analyze algorithms in the while.

[cols=",,",options="header",]
|===================================================================
|*Big O Notation* |*Name* |*Example(s)*
|O(1) |Constant |Finding if an array is empty or not
|O(log n) |Logarithmic |Element on sorted array with binary search
|O(n) |Linear |Duplicate elements in array with Hash Map
|O(n log n) |Linearithmic |Sorting elements in array with merge sort
|O(n^2^) |Quadratic |Duplicate elements in array (naïve),
|_O(n^3^)_ |Cubic |3 variables equation solver
|_O(2^n^)_ |Exponential |Find all subsets in a set
|_O(n!)_ |Factorial |Find all permutations of a word
|===================================================================

3

[[_Toc525822206]]Mastering Linear Data Structures Ins and Outs

Data Structures comes in many flavors. There’s no one to rule them all.
There are tradeoffs on each one of them. In real life you are not going
to be re-implementing them. However, knowing how they work internally
would help to know when to use one over another. We are going to explore
the most common data structures time and space complexity.

In this chapter we are going to learn about the following linear data
structures:

* _____
Array
_____
* ___________
Linked List
___________
* _____
Stack
_____

* Queue

Followed by graph-based data structures such as:

* Graph
* Tree
* Map
* Set

After this chapter, you will know the data structures trade-offs and
when to use one over the other.

= Array

Arrays in one of the most used data structure. It’s a collection of
things (strings, characters, numbers, objects, etc.). They can be many
or zero. Strings are a collection of Unicode characters and most of the
array concepts apply to them.

Fixed vs Dynamic Size Arrays

Some programming languages has fixed size arrays like Java and C++.
Fixed size arrays might be wasteful when you array gets filled and you
have to create a new one with bigger size. For that, those programming
languages also have built-in dynamic arrays: we have vector in C++ and
ArrayList in Java. Dynamic programming languages like JavaScript, Ruby,
Python use dynamic arrays by default.

Arrays look like this:

image:extracted-media/media/image16.png[image,width=388,height=110]

Figure 4 - Array representation: each value is accessed through an
index.

Arrays are a sequential collection of elements that can be accessed
randomly using an index. Let’s take a look into the different operations
that we can do with arrays.

== Insertion

Arrays are built-in into most languages. Inserting element in an array
is you can either:

{empty}(1) Add elements at the creation time:

_const_ array = [2, 5, 1, 9, 6, 7]; _// JavaScript_

_int_[] array = \{2, 5, 1, 9, 6, 7}; _// Java_

_int_ array[6] = \{2, 5, 1, 9, 6, 7}; _// C++_

or (2) initialize the array (empty)

_const_ array0 = []; _// JavaScript_

_int_[] array0 = _new_ _int_[6]; _// Java_

_int_ array0[6]; _// C++_

and add values later:

array0[2] = 1; // JS, Java, C++ is the same

Using the index, you would replace whatever value was there.

=== Inserting at the beginning of the array

What if you want to insert a new element at the beginning of the array?
You would have to push every element to the right.

// Insert to head, changes every index

array.unshift(0); //=> [0, 2, 5, 1, 9, 6, 7]

As you can see 2 was the index 0, now was pushed to index 1, and
everything else was pushed out. This takes *O(n)* since it affects all
the elements in the array.

JavaScript built-in array.unshift

The unshift() method adds one or more elements to the beginning of an
array and returns the new length of the array. Runtime: O(n).

=== Inserting at the middle of the array

Inserting a new element in the middle involves moving part of the array
but not all of the items.

// Inserting element sin the middle

array.splice(1, 0, 111); // at the position 1, delete 0 elements and
insert 111

//=> array: [2, 111, 5, 1, 9, 6, 7]

The Big O for this operation would be *O(n)*, since in worst case it
would move most of the elements to the right.

JavaScript built-in array.splice

The splice() method changes the contents of an array by removing
existing elements and/or adding new elements. Runtime: O(n).

=== Inserting at the end of the array

We can push new values to the end of the array like this:

// Insert to tail

array.push(4); // array: [2, 5, 1, 9, 6, 7, 4]

Adding to the tail of the array doesn’t change other indexes. E.g.
element 2 is still at index 0. So, this is a constant time operation
*O(1)*.

JavaScript built-in array.push

The push() method adds one or more elements to the end of an array and
returns the new length of the array. Runtime: O(1).

== Searching by value and index

Searching by index is very easy using the [] operator:

array[4]; _//=> 6_

This is takes a constant time, *O(1)*, to retrieve values out of the
array. If we want to get fancier we can create a function:

image:extracted-media/media/image17.png[image,width=528,height=293]

Finding out if an element is in the array or not is a different story.

image:extracted-media/media/image18.png[image,width=528,height=338]

We would have to loop through the whole array (worst case) or until we
find it. This takes *O(n)*.

== Deletion

Deleting like insertion there are three possible scenarios, deleting at
the beginning, middle or end.

=== Deleting element from the beginning

Deleting from beginning can be done using the splice funtion and also
the shift. Let’s use the latter since it’s simpler:

// Deleting from the beginning of the array.

array.shift(); //=> [5, 1, 9, 6, 7]

As expected, this will make every index to change, so this takes *O(n)*.

JavaScript built-in array.shift

The shift() method removes the first element from an array and returns
that removed element. This method changes the length of the array.
Runtime: O(n).

=== Deleting element from the middle

We can use the splice operator for this

// Deleting from the middle

array.splice(2, 1); // delete 1 element at position 2

// => array: [2, 5, 9, 6, 7]

This might cause most the elements of the array to move and occupied the
deleted position. Thus, runtime: O(n).

=== Deleting element from the end

Removing the last element is very straightforward:

// Deleting last element from the array

array.pop(); // => array: [2, 5, 1, 9, 6]

No element other element has be shifted so it’s a O(1) runtime.

JavaScript built-in array.pop

The pop() method removes the last element from an array and returns that
element. This method changes the length of the array. Runtime: O(1).

== Array Complexity

To sum up, the time complexity on an array is:

Table 4 - Time complexity of array operations

[cols=",,,,,,,,,",options="header",]
|=======================================================================
|Data Structure |Searching by |Inserting at the |Deleting from the
|Space Complexity | | | | |
| |_Index/Key_ |_Value_ |_beginning_ |_middle_ |_end_ |_beginning_
|_middle_ |_end_ |

|Array |O(1) |O(n) |O(n) |O(n) |O(1) |O(n) |O(n) |O(1) |O(n)
|=======================================================================

= Linked List

A list (or Linked List) is a linear data structure similar to array in
the sense it stores a collection of data. The difference, though, is
that it doesn’t use indexes.

== Singly Linked List

Each element or node is *linked* to the next one by a reference field.
This called *singly linked list*:

image:extracted-media/media/image19.png[image,width=498,height=97]

Figure 5 - Singly Linked List Representation: each node has a reference
(blue arrow) to the next one.

Usually, a list is represented by the first element in the list called
*head*. For instance, if you want to get the “cat” element, then the
only way is get there is using the next field on the head node. You
would get “dog” , then use again then next field and finally you get a
“cat”.

== Doubly Linked List

When you have also a reference to the previous element, then we have a
*doubly linked list*.

image:extracted-media/media/image20.png[image,width=528,height=74]

Figure 6 - Doubly Linked List: each node has a reference to the next and
previous element.

If we implement the code for the Node elements, it would be something
like this:

image:extracted-media/media/image21.png[image,width=528,height=285]

== Linked List vs Array

Since Linked Lists doesn’t have indexes you have to start from the
*first* element of the list, often called *root* or *head*. From the
root node, you follow the next reference recursively until you find the
node you are looking for or the end of the list. This takes O(n) to get
an element. You might be wondering… isn’t an array more efficient with
O(1) access time? It depends…

Arrays pre-allocates contiguous blocks of memory and if it outgrew that,
it has to copy all over to a bigger space. LinkedList’s nodes has
pointers. They don’t have to be next to each other nor large chunks of
memory have been reserved. Linked List is more like “grow as you go”.

Another advantage, is that adding/deleting at the beginning on an array
it takes O(n), however, in the linked list is a constant operation O(1)
as we will implement later.

A drawback of a linked list is that if you want to insert/delete an
element at the end of the list, you would have to navigate the whole
list to find the last one O(n). However, this can be solve by keeping
track of the last element and then inserting to the end or beginning is
constant: O(1).

So, let’s get started implementing the linked list, in our constructor
we keep a reference of the first (and last node for performance
reasons).

image:extracted-media/media/image22.png[image,width=528,height=251]

== Insertion

Similar to the array we have could add elements at the beginning, end or
anywhere in the middle of the list.

=== Inserting element at the beginning of the list

We are going to use the Node class to create a new element and stick it
at the beginning and make it your first.

image:extracted-media/media/image23.png[image,width=498,height=217]

Figure 7 – Insert at the beginning by linking the new node with the
current first node.

To insert at the beginning, we create a new node with the next reference
to the current first node. Then we make first the new node. In code, it
would look something like this:

/**

* Adds element to the begining of the list. Similar to Array.unshift

* Runtime: O(1)

* @param \{any} value

*/

addFirst(value) \{

const node = new Node(value);

node.next = this.first;

if (this.first) \{

this.first.previous = node;

} else \{

this.last = node;

}

this.first = node; // update head

this.size += 1;

return node;

}

As you can see, we create a new node and make it the first one.

=== Inserting element at the end of the list

Appending an element at the end of the list can be done very efficient
if we have a pointer to the last element in the list. Otherwise, you
would have to iterate through the whole list.

image:extracted-media/media/image24.png[image,width=498,height=208]

Figure 8 - Add last: get last node and reference next with the newly
created node. Then, update the last pointer.

In code:

/**

* Adds element to the end of the list (tail). Similar to Array.push

* Using the element last reference instead of navigating through the
list,

* we can reduced from linear to a constant runtime.

* Runtime: O(1)

* @param \{any} value node's value

* @returns \{Node} newly created node

*/

addLast(value) \{

const newNode = new Node(value);

if (this.first) \{

newNode.previous = this.last;

this.last.next = newNode;

this.last = newNode;

} else \{

this.first = newNode;

this.last = newNode;

}

this.size += 1;

return newNode;

}

If there’s no element in the list yet, the first and last node would be
the same. If there’s, then, we go to the last one and add a reference
next to the new node. That’s it! This is a constant time for both cases:
*O(1)*.

=== Inserting element at the middle of the list

For inserting an element at the middle of the list you would need
specify the position (index) in the list. Then, you create the new node
and update the references to it.

Let’s do an example, with a doubly linked list. We want to insert the
“new” node in the 2^nd^ position.

image:extracted-media/media/image25.png[image,width=528,height=358]

Figure 9 - Inserting node in the middle of a doubly linked list.

Here are the steps:

1.  Create the “new” node
2.  Point the “new” node next reference to the current element in the
2^nd^ position and previous to the node in the 1^st^ position. However,
no other node is pointing to it in the list. Let’s fix that.
3.  Change 1^st^ node next reference from “dog” to “new”.
4.  Change “dog” node previous reference from “art” to “new”.

Let’s work in the code to do this:

_/**_

_* Insert new element at the given position (index)_

_*_

_* @param \{any} value new node's value_

_* @param \{Number} position position to insert element_

_* @returns \{Node} new node or 'undefined' if the index is out of
bound._

_*/_

add(value, position = 0) \{

_if_ (position === 0) \{

_return_ _this_.addFirst(value);

}

_if_ (position === _this_.size) \{

_return_ _this_.addLast(value);

}

_for_ (_let_ current = _this_.first,

index = 0;

index <= _this_.size;

index += 1,

current = (current && current.next)) \{

_if_ (index === position) \{

_const_ newNode = new Node(value);

newNode.previous = current.previous;

newNode.next = current;

current.previous.next = newNode;

_if_ (current.next) \{ current.next.previous = newNode; }

_this_.size += 1;

_return_ newNode;

}

}

_return_ undefined; _// out of bound index_

}

Take notice that we reused, addFirst and addLast functions. For all the
other cases the insertion is on the middle. We use current.previous.next
and current.next.previous to update the surrounding elements references
with the new node. This one takes *O(n)* because we have to iterate
through the list.

== Searching by value

Finding an element by value there’s no other way than iterating through
the whole list.

/**

* Search by value. It finds first occurrence of

* the element matching the value.

* Runtime: O(n)

* @param \{any} value

* @returns \{number} return index or undefined

*/

indexOf(value) \{

for (let current = this.first, index = 0;

current;

index += 1, current = current.next) \{

if (current.value === value) \{

return index;

}

}

return undefined; // not found

}

If we find the element the method will return the index otherwise
undefined. Runtime: O(n).

== Searching by index

Searching by index is very similar, we iterate throught the list until
we find the element that matches the position

/**

* Search by index

* @param \{Number} index position of the element

* @returns \{Node} element at the specified position in this list.

*/

get(index = 0) \{

for (let current = this.first, position = 0;

current;

position += 1, current = current.next) \{

if (position === index) \{

return current;

}

}

return undefined; // not found

}

If there’s no match, we return undefined then. The runtime is O(n). As
you might notice the search by index and by position methods looks
pretty similar. If you want to take a look at the refactored version
check out:
https://github.com/amejiarosario/algorithms.js/blob/master/src/data-structures/linked-lists/linked-list.js

== Deletion

Deleting is an interesting one. We don’t actually delete an element, we
just remove all reference to it. Let’s go case by case to explore what
happens.

=== Deleting element from the head

Deleting the first element (or head) is just removing all references to
it.

image:extracted-media/media/image26.png[image,width=528,height=74]

Figure 10 - Deleting an element from the head of the list

For instance, to remove the head (“art”) node, you assign first to the
second node “dog” and remove the previous reference to the “art” node.
The garbage collector will take care of “art”, when it sees nobody is
using it anymore.

In code, it looks like this:

/**

* Removes element from the start of the list (head/root).

* Similar to Array.shift

* Runtime: O(1)

* @returns \{any} the first element's value which was removed.

*/

removeFirst() \{

const head = this.first;

if (head) \{

this.first = head.next;

if (this.first) \{

this.first.previous = null;

}

this.size -= 1;

} else \{

this.last = null;

}

return head && head.value;

}

As you can see, when we want to remove the first node

=== Deleting element from the tail

Removing the last element from the list would require to iterator from
the head until we find the last one, that’s O(n). We have a reference to
the last element, which we do! So, we can do it in O(1).

image:extracted-media/media/image27.png[image,width=528,height=221]

Figure 11 - Removing last element from the list using the last
reference.

For instance, if we want to remove last node “cat”. We use the last
pointer to avoid iterating through the whole list. We check
last.previous to get the “dog” node and made it the new last and remove
its next reference to “cat”. Since, nothing is pointing to “cat” then is
out of the list.

Let’s code this up like this:

/**

* Removes element to the end of the list. Similar to Array.pop

* Using the `last.previous` we can reduce the runtime from O(n) to O(1)

* Runtime: O(1)

* @returns \{value} the last element's value which was removed

*/

removeLast() \{

const tail = this.last;

if (tail) \{

this.last = tail.previous;

if (this.last) \{

this.last.next = null;

} else \{

this.first = null;

}

this.size -= 1;

}

return tail && tail.value;

}

The code is very similar to removeFirst, but instead of first we update
last reference and instead of nullifying previous we nullify its next
reference.

=== Deleting element from the middle

To remove a node from the middle, we make the surrounding nodes to
bypass the one we want to delete.

image:extracted-media/media/image28.png[image,width=528,height=259]

Figure 12 - Remove middle node making their surrounding nodes bypass the
node we want to remove.

In the illustration, we are removing the middle node “dog” by making
art’s next to be cat and cat’s previous to be “art” totally bypassing
“dog”.

Let’s implement it:

/**

* Removes the element at the specified position in this list.

* Runtime: O(n)

* @param \{any} position

* @returns \{any} the element's value at the specified position that was
removed.

*/

remove(position = 0) \{

const current = this.get(position);

if (position === 0) \{

this.removeFirst();

} else if (position === this.size) \{

this.removeLast();

} else if (current) \{

current.previous = current.next;

this.size -= 1;

}

return current && current.value;

}

Notice that we are using the get method to get the node at the current
position. That method loops throught the list until it found the node at
the specified position. This is an O(n).

== Linked List Complexity vs Array Complexity

So far, we have seen two liner data structures with different use cases.
Here’s a summary:

Table 5 - Big O cheat sheet for Linked List and Array

[cols=",,,,,,,,,",options="header",]
|=======================================================================
|*Data Structure* |*Searching by* |*Inserting at the* |*Deleting from
the* |*Space Complexity* | | | | |
| |_*Index/Key*_ |_*Value*_ |_*start*_ |_*middle*_ |_*end*_ |_*start*_
|_*middle*_ |_*end*_ |

|Array |*O(1)* |*O(n)* |*O(n)* |*O(n)* |*O(1)* |*O(n)* |*O(n)* |*O(1)*
|*O(n)*

|Linked List (singly) |*O(n)* |*O(n)* |*O(1)* |*O(n)* |*O(1)* |*O(1)*
|*O(n)* |*O(n)* |*O(n)*

|Linked List (doubly) |*O(n)* |*O(n)* |*O(1)* |*O(n)* |*O(1)* |*O(1)*
|*O(n)* |*O(1)* |*O(n)*
|=======================================================================

If you compare singly linked list vs doubly linked list, you will notice
that main difference is deleting elements from the end. For a singly
list is *O(n)*, while for a doubly list is *O(1)*.

Comparing an array with a doubly linked list, both have different use
cases:

Use arrays when…

* You want to access *random* elements by numeric key or index in
constant time O(1).
* Arrays can be single dimensional, two-dimensional and
multi-dimensional.

Use a doubly linked lists when…

* You want to access elements in a *sequential* manner.

* Lists can be singly, doubly and circular (last element points to the
first one).

* You want to insert elements at the start and end of the list. Linked
list has O(1) while array has O(n).
* You want to save some memory when dealing with possibly large data
sets. Arrays pre-allocate a large chunk of contiguous memory on
initialization. Lists are more “grow as you go”.

For the next two linear data structures Stack and Queue, we are going to
use doubly linked list to implement them. We could use an array as well
but since inserting/deleting from the start perform better on
linked-list we are going use that.

= Stack

Stack is a data structure that restrict the way you add and remove data
to it. It only allows you to insert and remove in a *Last-In-First-Out*
(LIFO) fashion. It has multiple

image:extracted-media/media/image29.png[image,width=240,height=238]

Figure 13 – Stack data structure is like a stack of disks: last element
in, is the first element out

Change image from
https://www.khanacademy.org/computing/computer-science/algorithms/towers-of-hanoi/a/towers-of-hanoi[Khan
Academy]

You can think of it as a stack of disks that you put into a rod. If you
insert the disks in the order 5, 4, 3, 2, 1. Then you can remove them on
1, 2, 3, 4, 5.

The stack inserts to the end of the list and also removes from the end.
Both, an array and linked list would do it in constant time. However,
since we don’t need the Array’s random access in the data a linked list
sequential data makes more sense.

_class_ Stack \{

_constructor_() \{

_this_.items = new LinkedList();

}

}

== Insertion

We can insert into an stack using the linked list’s addLast.

/**

* Add element into the stack.

* Similar to Array.push

* Runtime: O(1)

* @param \{any} item

*/

add(item) \{

this.items.addLast(item);

return this;

}

We are returning this, in case we want to chain multiple add commands.

== Deletion

Deleting is straightforward as well.

/**

* Remove element from the stack.

* Similar to Array.pop

* Runtime: O(1)

*/

remove() \{

return this.items.removeLast();

}

This time we used linked list’s removeLast. That’s all we need for a
stack implementation. Check out the full file at
https://github.com/amejiarosario/algorithms.js/blob/master/src/data-structures/stacks/stack.js

== Implementation Usage

We can use our stack implementation as follows:

_const_ stack = new Stack();

stack.add('a');

stack.add('b');

stack.remove(); _// b_

stack.add('c');

stack.remove(); _// c_

stack.remove(); _// a_

As you can see if we add new items they will be the first to go out to
honor LIFO.

== Stack Complexity

Implementing the stack with an array and linked list would lead to the
same time complexity:

[cols=",,,,,,,,,",options="header",]
|=======================================================================
|Data Structure |Searching by |Inserting at the |Deleting from the
|Space Complexity | | | | |
| |_Index/Key_ |_Value_ |_start_ |_middle_ |_end_ |_start_ |_middle_
|_end_ |

|Stack |- |- |- |- |*O(1)* |- |- |*O(1)* |*O(n)*
|=======================================================================

There’s no really a use case for searching values on a stack, usually
you only check the value when you remove it.

= Queue

Queue is a linear data structure where the data flows in a
*First-In-First-Out* (FIFO) manner.

image:extracted-media/media/image30.png[image,width=528,height=171]

Figure 14 – Queue data structure is like a line of people: the First-in,
is the First-out

Queue is like a line of people, the first one to get in the line is the
first out as well. Similar to the stack, we only have to operations. In
a Queue, we insert elements to the back of the list and remove it from
the front.

We could use an array or a linked list to implement a Queue. However, is
recommended to only use linked list. An array has a runtime of O(n) to
remove element from the start while a list is constant O(1).

_class_ Queue \{

_constructor_() \{

_this_.items = new LinkedList();

}

}

We initialize the Queue creating a linked list. Now, let’s add the
enqueue and dequeue methods.

== Insertion

For enqueue, we add elements to the back of the list:

/**

* Add element to the queue

* Runtime: O(1)

* @param \{any} item

*/

enqueue(item) \{

this.items.addLast(item);

}

== Deletion

For dequeue, we remove elements from the front of the list:

/**

* Remove element from the queue

* Runtime: O(1)

*/

dequeue() \{

return this.items.removeFirst();

}

== Implementation usage

We can use our Queue class like follows:

_const_ queue = new Queue();

queue.enqueue('a');

queue.enqueue('b');

queue.dequeue(); _// a_

queue.enqueue('c');

queue.dequeue(); _// b_

queue.dequeue(); _// c_

You can see that the items are dequeue in the same order they were
added.

== Queue Complexity

A mode of experiment we can see in the following table that if we would
have implemented the Queue using an array it’s enqueue time would be
O(n) instead of O(1). Check it out.

Table 6 - Time complexity for queue operations

[cols=",,,,,,,,,",options="header",]
|=======================================================================
|Data Structure |Searching by |Inserting at the |Deleting from the
|Space Complexity | | | | |
| |_Index/Key_ |_Value_ |_start_ |_middle_ |_end_ |_start_ |_middle_
|_end_ |

|Queue (w/array) |- |- |*O(n)* |- |- |- |- |*O(1)* |*O(n)*

|Queue (w/list) |- |- |*O(1)* |- |- |- |- |*O(1)* |*O(n)*
|=======================================================================

That’s all for queues!

= Summary

In this chapter we explored the most used linear data structures such as
Arrays, Lists, Stacks and Queues. We implemented them and discussed the
runtime of their operations.

To sum up,

Use Arrays when…

* You need to access data in random order fast (using an index).
* Your data is multi-dimensional (e.g. matrix, tensor).

Use Linked Lists when…

* You will access your data sequentially.
* You want to save memory and only allocate memory as you need it.

Use Queues when…

* You need to access your data in a first-come, first-served basis.

Use Stacks when…

* You need to access your data first-in, last-out (FIFO)

Table 7 - Time complexity of Array, LinkedList, Stack and Queues

[cols=",,,,,,,,,",options="header",]
|=======================================================================
|Data Structure |Searching by |Inserting at the |Deleting from the
|Space Complexity | | | | |
| |_Index/Key_ |_Value_ |_start_ |_middle_ |_end_ |_start_ |_middle_
|_end_ |

|Array |*O(1)* |*O(n)* |*O(n)* |*O(n)* |*O(1)* |*O(n)* |*O(n)* |*O(1)*
|*O(n)*

|Linked List (singly) |*O(n)* |*O(n)* |*O(1)* |*O(n)* |*O(1)* |*O(1)*
|*O(n)* |*O(n)* |*O(n)*

|Linked List (doubly) |*O(n)* |*O(n)* |*O(1)* |*O(n)* |*O(1)* |*O(1)*
|*O(n)* |*O(1)* |*O(n)*

|Stack |- |- |- |- |*O(1)* |- |- |*O(1)* |*O(n)*

|Queue (w/array) |- |- |*O(n)* |- |- |- |- |*O(1)* |*O(n)*

|Queue (w/list) |- |- |*O(1)* |- |- |- |- |*O(1)* |*O(n)*
|=======================================================================

In the next chapter, we are going to explore non-linear data structure
like HashMaps, Graphs and Trees.

4

[[_Toc525822212]]Demystifying Non-Linear Data Structures

Non-Linear data structures are everywhere weather we realize it or not.
They are used in databases, Web (HTML DOM tree), search algorithms,
finding best route to get home and so on. In this chapter, we are going
to learn the basic concepts and when to choose one over the other.

In this chapter we are going to learn:

* __________________________________________
Exciting Graph data structure applications
__________________________________________
* _______________________________________________
Searching efficiently with Tree data structures
_______________________________________________
* _________________________________________________________
One of the most versatile data structure of all Hash Maps
_________________________________________________________
* __________________________
Keeping dups out with Sets
__________________________

= Tree Data Structure

A tree is a non-linear data structure where node can have zero or more
nodes. The first node in a tree is called *root*. The linked nodes to
the root is called *children* or *descendants*.

image:extracted-media/media/image31.jpg[image,width=404,height=240]

Figure 15 - Tree Data Structure: root node and descendants.

They are called “trees” because the data structure resembles a tree . It
starts with a *root* node and *branch* off with its descendants, and
finally, there are *leaves*.

== Implementing a Tree

Implementing a tree is not that hard. It’s similar to a Linked List but
the difference is that instead of having only a next and previous links,
we have an array of linked nodes (children).

_class_ TreeNode \{

_constructor_(value) \{

_this_.value = value;

_this_.descendents = [];

}

}

Simple! Right? But there are some constraints that you have to keep at
all times:

1.  You have to be careful to not do a circular loop, otherwise this
wouldn’t be a tree but a *graph data structure*! E.g. Node A has B as
child, then Node B list Node A as its descendant forming a loop. ‍️
2.  A node with more than two parents. If that happens is no longer a
tree but a graph.
3.  A tree must have only one root. Two non-connected parts are not a
tree. Graph can have non-connected parts and doesn’t have root.

== Basic concepts

Here’s summary of the tree basic concepts:

* The top most node is called *root*.
* A node’s immediate linked nodes are called *children*.
* A *leaf* or *terminal node* is a node without any descendent or
children.
* A node immediate ancestor is called *parent*. Yeah, like a family tree
‍‍‍ you can have *uncles* and *siblings*, and *grandparents*.
* *Internal nodes* are all nodes except for the leaf nodes and root
node.
* The connection/link between nodes is called *edge*.
* The *height of a tree* is the distance, edge count, from the farthest
leaf to the root. You can also calculate the *height of a node* counting
the edges between them and the farthest leaf. For instance, from the
image above:

* Node A, has a height of 3.
* Node G has a height of 1.
* Node I, has a height of 0.

* The *depth* *of a tree* is the distance, edge count, from the root to
the farthest leaf.

== Types of Binary Trees

There are different kinds of trees depending on the restrictions. E.g.
The trees that have at most two children are called *binary tree*, while
trees with at most 3 children are called *Ternary Tree*. Since binary
trees are most common we are going to cover them here and ternary tree
and others in another chapter.

=== Binary Tree

The binary restricts the nodes to have at most 2 children. Trees in
general can have 3, 4, 26 or more, but not binary trees.

image:extracted-media/media/image32.png[image,width=321,height=193]

Figure 16 - Binary tree has at most 2 children while non-binary trees
can have more.

=== Binary Search Tree (BST)

BST has the same restriction as binary tree; each node has at most 2
children. Also, there’s another restriction: the left child’s value has
to be less than the parent and the right child’s value has be bigger
than the parent.

image:extracted-media/media/image33.png[image,width=348,height=189]

Figure 17 - BST left < parent < right, while non-BST can be in any
order.

=== Binary Heap

The heap is a type of binary tree where the children values are higher
(max-heap) than the parent but contrary to the BST the left child
doesn’t have to be smaller than the right child.

image:extracted-media/media/image34.png[image,width=325,height=176]

Figure 18 - Heap vs BST, has max/min value in the root, while BST
doesn’t.

For a *max-heap*, the root has the highest value. The heap guarantee as
you go down by level the elements on higher levels has bigger values
than the element on lower levels. The opposite is true for a *min-heap*.
In a min-heap the lowest value is at the root and as you go down the
lower levels has higher values than the ones above.

image:extracted-media/media/image35.png[image,width=258,height=169]

Figure 19 - max-heap keeps highest value at the top while min-heap keep
the lowest at the root.

Heap vs Binary Search Tree (BST)

Heap is better at finding max or min values in constant time *O(1)*,
while a balanced BST is good a finding any element in *O(log n)*. Heaps
are often used to implement priority queues while BST is used when you
need every value sorted.

== Implementing a Binary Search Tree

The BST implementation has to keep a couple of constraints:

* Each node must have at most 2 children. Usually referred as left and
right.
* All trees must a have a root node.
* Nodes values must be ordered after each insert/delete operation.
* The order of nodes values must be: left child < parent < right child.

The first step is to implement the TreeNode:

_class_ BinaryTreeNode \{

_constructor_(value) \{

_this_.value = value;

_this_.meta = \{};

_this_.left = null;

_this_.right = null;

}

}

Does this look familiar to you? It’s almost like the linked list node,
but instead of having next and previous, it has left and right. That
guarantees that we have at most two children.

We also added the meta object to hold some metadata about the node, like
duplicity, color (for red-black trees), or any other data needed for
future algorithms.

We implemented the node, now let’s go with the tree:

_class_ BinarySearchTree \{

_constructor_() \{

_this_.root = null;

_this_.size = 0;

}

add(value) \{ _/* ... */_ }

find(value) \{ _/* ... */_ }

remove(value) \{ _/* ... */_ }

getMax() \{ _/* ... */_ }

getMin() \{ _/* ... */_ }

}

This guarantees that our trees have one root element from where we can
navigate left or right based on the value that we are looking for. We
have placeholders for the operations that we are going to implement in a
moment.

=== Inserting new elements in a BST

For inserting an element in a BST we have two scenarios:

1.  If tree is empty (root element is null), we add the newly created
node as a root and we are done!
2.  Starting from the root, compare the node’s value against the new
element. If node has higher than new element, we move to the right
child, otherwise to the left. We check recursively each node until we
find an empty spot where we can put the new element and keep the rule
right < parent < left.
3.  If we insert the same value multiple times, we don’t want
duplicates. So, we can keep track using a duplicity counter.

For instance, let’s say that we want to insert the values 19,21,10,2,8
in a BST:

image:extracted-media/media/image36.png[image,width=528,height=329]

Figure 20 - Inserting values on a BST.

In the last box, when we are inserting node 18, we start by the root
(19). Since 18 is less than 19, then we move left. Node 18 is greater
than 10, so we move right. There’s an empty spot and we place it there.
Let’s code it up:

/**

* Insert value on the BST.

* If the value is already in the tree, t

* then it increase the multiplicity value

* @param \{any} value value to insert in the tree

*/

add(value) \{

const newNode = new TreeNode(value);

if (this.root) \{

const \{ found, parent } = this.findNodeAndParent(value);

if (found) \{ // duplicated: value already exist on the tree

found.meta.multiplicity = (found.meta.multiplicity || 1) + 1;

} else if (value < parent.value) \{

parent.left = newNode;

} else \{

parent.right = newNode;

}

} else \{

this.root = newNode;

}

this.size += 1;

return newNode;

}

Two things to point out about our add method:

1.  We are taking care of duplicates. Instead of inserting duplicates we
are keeping a multiplicity tally. We have to decrease it when removing
nodes.
2.  We are using a helper function findNodeAndParent to iterate through
the tree finding a node with current value “found” and its parent.

=== Finding a value in a BST

We can implement the find method using the helper findNodeAndParent as
follows:

/**

* Return node if it found it or undefined if not

* @param \{any} value value to find

*/

find(value) \{

return this.findNodeAndParent(value).found;

}

/**

* Finds the node matching the value.

* If it doesn't find, it returns the leaf where the new value should be
added.

* @param \{any} value Node's value to find

* @returns \{TreeNode} matching node or the previous node where value
should go

*/

findNodeAndParent(value, node = this.root, parent = null) \{

if (!node || node.value === value) \{

return \{ found: node, parent };

} else if (value < node.value) \{

return this.findNodeAndParent(value, node.left, node);

}

return this.findNodeAndParent(value, node.right, node);

}

findNodeAndParent is a recursive function that goes to the left child or
right depending on the value. However, if the value already exists it
will return it in found variable.

=== Removing elements from a BST

Deleting a node from a BST have three cases, the node is a 1) leaf, 2)
parent with one child, 3) parent with two children/root.

==== Removing a leaf (Node with 0 children)

Deleting a leaf is the easiest, we just look for their parent and set
the child to null.

image:extracted-media/media/image37.png[image,width=528,height=200]

Figure 21 - Removing node without children from a BST.

Node 18, will be hanging around until the garbage collector is run.
However, there’s no node referencing to it so it won’t be reachable from
the tree anymore.

==== Removing a parent (Node with 1 children)

Removing a parent is not as easy, since you need to find new parents for
its children.

image:extracted-media/media/image38.png[image,width=528,height=192]

Figure 22 - Removing node with 1 children from a BST.

In the example, we removed node 10 from the tree so its child (node 2)
needs a new parent. We made node 19 the new parent for node 2.

==== Removing a full parent (Node with 2 children) or root

This is the trickiest of all cases because we need to find new parents
for two children.

image:extracted-media/media/image39.png[image,width=528,height=404]

Figure 23 - Removing node with two children from a BST.

In the example, we delete the root node 19. This leaves the two orphans
(node 10 and node 21). There’s no more parents because node 19 was the
*root* element. One way to solve this problem, is to *combine* the left
subtree (Node 10 and descendants) into the right subtree (node 21). The
final result is node 21 is the new root.

What would happen if node 21 had a left child (e.g. node 20)? Well, we
would move node 10 and its descendants bellow node 20.

==== Implementing removing elements from a BST

All the described scenarios removing nodes with 0, 1 and 2 children can
be sum up on this code:

/**

* Remove a node from the tree

* @returns \{boolean} false if not found and true if it was deleted

*/

remove(value) \{

const \{ found: nodeToRemove, parent } = this.findNodeAndParent(value);

if (!nodeToRemove) return false;

// Combine left and right children into one subtree without nodeToRemove

const removedNodeChildren =
this.combineLeftIntoRightSubtree(nodeToRemove);

if (nodeToRemove.meta.multiplicity && nodeToRemove.meta.multiplicity >
1) \{

nodeToRemove.meta.multiplicity -= 1; // handles duplicated

} else if (nodeToRemove === this.root) \{

// Replace (root) node to delete with the combined subtree.

this.root = removedNodeChildren;

this.root.parent = null; // clearing up old parent

} else \{

const side = nodeToRemove.isParentLeftChild ? 'left' : 'right';

// Replace node to delete with the combined subtree.

parent[side] = removedNodeChildren;

}

this.size -= 1;

return true;

}

We first try to find if the node exists if it doesn’t we are done! If it
does exist, then we check the multiplicity (duplicates) and decrement
the count in case we have multiple nodes with the same value.

Later, we compute removedNodeChildren, which is the resulting subtree
after combining the children of the deleted node.

If the nodeToRemove was the root, then we move the removed node’s
children as the new root. If it was not the root, then we go to the
deleted node’s parent and put their children there.

The method to combine subtrees is the following:

/**

* Combine left into right children into one subtree without given parent
node.

*

* @example combineLeftIntoRightSubtree(30)

*

* 30* 40

* / \ / \

* 10 40 combined 35 50

* \ / \ ----------> /

* 15 35 50 10

* \

* 15

*

* It takes node 30 left subtree (10 and 15) and put it in the

* leftmost node of the right subtree (40, 35, 50).

*

* @param \{TreeNode} node

* @returns \{TreeNode} combined subtree

*/

combineLeftIntoRightSubtree(node) \{

if (node.right) \{

const leftmost = this.getLeftmost(node.right);

leftmost.left = node.left;

return node.right;

}

return node.left;

}

Take a look at code above and the example. You will see how to remove
node 30 and combine both children subtree and keeping the BST rules.
Also, this method uses a helper to get the left-most node. We can
implement it like this:

/**

* Get the node with the min value of subtree: the left-most value.

* @param \{TreeNode} node subtree's root

* @returns \{TreeNode} left-most node (min value)

*/

getLeftmost(node = this.root) \{

if (!node || !node.left) \{

return node;

}

return this.getMin(node.left);

}

That’s all we need to remove elements from a BST. Check out the complete
BST implementation at:
https://github.com/amejiarosario/algorithms.js/blob/master/src/data-structures/trees/binary-search-tree.js

== Differentiating a balanced and non-balanced Tree

As we insert and remove nodes from a BST we could ended up like the tree
on the left:

image:extracted-media/media/image40.png[image,width=454,height=201]

Figure 24 - Balanced vs Unbalanced Tree.

The tree on the left is unbalanced. It actually looks like a Linked List
and has the same runtime! Searching for an element would be *O(n)*.
However, on a balanced tree the search time is *O(log n)*. That’s why we
always want to keep the tree balanced. In further chapters, we are going
to explore how to keep a tree balanced after each insert/delete.

== Tree Complexity

We can sum up the tree operations using Big O notation:

[cols=",,,,,",options="header",]
|=======================================================================
|Data Structure |Searching by |Insert |Delete |Space Complexity |
| |_Index/Key_ |_Value_ | | |

|Binary Search Tree (unbalanced) |- |*O(n)* |*O(n)* |*O(n)* |*O(n)*

|Binary Search Tree (balanced) |- |*O(log n)* |*O(log n)* |*O(log n)*
|*O(n)*
|=======================================================================

= Map

A map is a data structure to store pairs of data: *key* and *value*. In
an array, you can only store values. The array’s key is always the
position index. However, in a *Map* the key can be whatever you want.

Map is a data structure that _maps_ *keys* to *values*.

Many languages have maps already built-in. This is an example in
JavaScript/Node:

_const_ myMap = new Map();

_// mapping values to keys_

myMap.set('string', 'foo');

myMap.set(1, 'bar');

myMap.set(\{}, 'baz');

_const_ obj1 = \{};

myMap.set(obj1, 'test');

_// searching values by key_

myMap.get(1); _//=> bar_

myMap.get('str'); _//=> foo_

myMap.get(\{}); _//=> undefined_

myMap.get(obj1); _//=> test_

The attractive part of Maps is that they are very performant usually
*O(1)* or *O(log n)* depending on the implementation. We can implement
the maps using two different techniques:

* *HashMap*: it’s a map implementation using an *array* and *hash
function*. The job of the hash function is to convert the key into an
index that contains the matching data. Optimized HashMap can have an
average runtime of *O(1)*.
* *TreeMap*: it’s a map implementation that uses a self-balanced Binary
Search Tree (red-black tree). The BST nodes store the key and the value
and nodes are sorted by key to guarantee an *O(log n)* look up.

== HashMap vs TreeMap

Here are the key differences:

* HashMap is more time-efficient. A TreeMap is more space-efficient.
* TreeMap search complexity is *O(log n)*, while an optimized HashMap is
*O(1)* on average. 
* HashMap’s keys are in insertion order (or random in some
implementations). TreeMap’s keys are always sorted.
* TreeMap offers some statistical data for free such as: get minimum,
get maximum, median, find ranges of keys. HashMap doesn’t.
* TreeMap has a guarantee always a *O(log n)*, while HashMaps has a
amortized time of *O(1)* but in the rare case of a rehash it would take
a *O(n)*.

== Learning how hash maps work

A HashMap is composed of two things: 1) a hash function and 2) a bucket
array to store values. Before going into the implementation details
let’s give an overview how it works. Let’s say we want to keep a tally
of things:

_const_ hashMap = new HashMap();

hashMap.set('cat', 2);

hashMap.set('art', 8);

hashMap.set('rat', 7);

hashMap.set('dog', 1);

How are keys map to their values? Here’s an illustration:

image:extracted-media/media/image41.png[image,width=528,height=299]

Figure 25 - HashMap representation. Keys are mapped to values using a
hash function.

This is the main idea:

1.  We use a *hash function* to transform the keys (e.g. dog, cat, rat,
…) into an array index. This array is called *bucket*.

1.  The bucket holds the values (linked list in case of collisions).

In the illustration, we have a bucket size of 10. In the bucket 0, we
have a collision. Both, cat and art keys are mapped to the same bucket
even thought their hash codes are different.

In a HashMap, a *collision* is when different keys are mapped to the
same index. They are bad for performance since it can reduce the search
time from *O(1)* to *O(n)*.

Having a big bucket size can avoid collision but also can waste too much
memory. We are going to build an _optimized_ HashMap that re-sizes
itself when is getting full. This avoid collisions and doesn’t waste too
much memory upfront. Let’s start with the hash function.

=== Designing an optimized hash function

In order to minimize collisions, we need to create a great hash
function.

A *perfect* hash function is one that assign a unique array index for
every different key.

It’s hard and memory-wise wasteful to have a perfect has function so we
are going to shot for a great hash function. To recap:

A hash function converts keys into array indices.

A hash function is composed of two parts:

1.  *Hash Code*: maps any key into an integer (unbonded)
2.  *Compression function*: maps an arbitrary integer to integer in the
range of [0… BUCKET_SIZE -1].

==== Analysing collisions on bad hash code functions

The goal of a hash code function is to convert any value given into a
positive integer. A common way to accomplish with summing each string’s
Unicode value.

_function_ hashCode(key) \{

_return_ Array.from(key.toString()).reduce((hashCode, char) _=>_ \{

_return_ hashCode + char.codePointAt(0);

}, 0);

}

This function uses codePointAt to get the Unicode value. E.g. a has a
value of 97, A is 65, even
https://en.wikipedia.org/wiki/Emoji#Unicode_blocks[emojis have codes];
“” is 128513.

JavaScript built-in string.charCodeAt and string.codePointAt

The charCodeAt() method returns an integer between 0 and 65535
representing the UTF-16 code unit at the given index. However, it
doesn’t play nice with Unicode, so it’s better to use codePointAt
instead.

The codePointAt() method returns a non-negative integer that is the
Unicode code point value.

With this function we have the can convert some keys to numbers as
follows:

hashCode('cat'); _//=> 312 (c=99 + a=97 + t=116)_

hashCode('dog'); _//=> 314 (d=100 + o=111 + g=103)_

hashCode('rat'); //=> 327 (r=114 + a=97 + t=116)

hashCode('art'); //=> 327 (a=97 + r=114 + t=116)

hashCode(10); _//=> 97_ _('1'=49 + '0'=48)_

hashCode('10'); _//=> 97 ('1'=49 + '0'=48)_

Notice that rat and art have the same hash code! Also, if we have the
string or number 10 they also should produce different output. These are
collisions that we need to solve.

This happened because we just summing the char codes and are not taking
the order into account nor the type. We can do this by offsetting the
char value based on their position in the string and appending the type
into the calculation.

/**

* Calculates polynomial hash code that maps a key (value) to an integer
(unbounded).

* It uses a 20 bit offset to avoid Unicode value overlaps

* @param \{any} key

* @returns \{BigInt} returns big integer (unbounded) that maps to the
key

*/

function hashCode(key) \{

const array = Array.from(`$\{key}$\{typeof key}`);

return array.reduce((hashCode, char, position) => \{

return hashCode + BigInt(char.codePointAt(0)) * (2n ** (BigInt(position)
* 20n));

}, 0n);

}

Since Unicode uses 20 bits, we can offset each character by 20 bits
based on the position.

JavaScript built-in BigInt

BigInt allows to operate beyond the maximum safe limit of integers
(Number.MAX_SAFE_INTEGER => 9,007,199,254,740,991). BigInt uses the
suffix n, e.g. 1n + 3n === 4n.

As you can imagine the output is a humongous number! We are using BigInt
that doesn’t overflow.

hashCode(10) === hashCode('10'); _//=> false_

hashCode('10') === hashCode('10string'); _//=> false_

hashCode('art') === hashCode('rat'); _//=> false_

hashCode('😄') === hashCode('😄'); _//=> true_

hashCode('😄') === hashCode('😸'); _//=> false_

We don’t have duplicates if the keys have different content or type.
However, we need to represent these unbounded integers. We do that using
*compression function* they can be as simple as % BUCKET_SIZE.

However, there’s an issue with the last implementation. It doesn’t
matter how big is the number (we are using BigInt), if we at the end use
the modulus to get an array index, then the part of the number that
truly matters is the last bits. Also, the modulus itself is much better
if is a prime number.

Look at this example with a bucket size of 4.

10 % 4 //↪️ 2

20 % 4 //↪️ 0

30 % 4 //↪️ 2

40 % 4 //↪️ 0

50 % 4 //↪️ 2

We get many collisions. Let’s see what happens if the bucket size is a
prime number:

10 % 7 //↪️ 3

20 % 7 //↪️ 6

30 % 7 //↪️ 2

40 % 7 //↪️ 4

50 % 7 //↪️ 1

Now it’s more evenly distributed!!

SO, to sum up:

* Bucket size should always be a prime number so data is distributed
more evenly and minimized collisions.
* Hash code doesn’t have to be too big. At the end what matters is the
few last digits.

Let’s design a better HashMap with what we learned.

==== Implementing an optimized hash function

Take a look at the following function

/**

* Polynomial hash codes are used to hash string typed keys.

* It uses FVN-1a hashing algorithm for 32 bits

* @see
https://en.wikipedia.org/wiki/Fowler%E2%80%93Noll%E2%80%93Vo_hash_function

* @param \{any} key

* @return \{integer} bucket index

*/

 hashFunction(key) \{

const str = String(key);

let hash = 2166136261; // FNV_offset_basis (32 bit)

for (let i = 0; i < str.length; i += 1) \{

hash ^= str.codePointAt(i); // XOR

hash *= 16777619; // 32 bit FNV_prime

}

return (hash >>> 0) % this.buckets.length;

}

Is somewhat similar to what we did before, in the sense that we use each
letter’s Unicode is used to compute the hash. The difference is:

1.  We are using a the XOR bitwise operation (^) to produce an
*avalanche effect*, where a small change in two strings produces
completely different hash codes. E.g.

fnv1a('cat') //↪️ 4201630708

fnv1a('cats') //↪️ 3304940933

1.  We are using FVN-1a prime numbers and offset to reduce collisions
even further. Check the
https://en.wikipedia.org/wiki/Fowler%E2%80%93Noll%E2%80%93Vo_hash_function[link]
to see where this prime numbers and offsets come from.

This hash function is a good trade-off between speed and collision
prevention.

Fowler/Noll/Vo (FNV) Hash

It is a non-cryptographic hash function designed to be fast while
maintaining low collision rate. The high dispersion of the FNV hashes
makes them well suited for hashing nearly identical strings such as
URLs, keys, IP addresses, zip codes and others.

Now that we have a good hash function. Let’s move on with the rest of
the HashMap implementation.

== Implementing a HashMap in JavaScript

Let’s start by creating a class and its constructor to initialize the
hash map. We want an array called *buckets* to hold all the data.

class HashMap \{

constructor(initialCapacity = 19, loadFactor = 0.75) \{

this.initialCapacity = initialCapacity;

this.loadFactor = loadFactor;

this.buckets = new Array(this.initialCapacity);

this.size = 0;

this.collisions = 0;

}

getLoadFactor() \{

return this.size / this.buckets.length;

}

isBeyondloadFactor() \{

return this.getLoadFactor() > this.loadFactor;

}

}

Notice that we are also keeping track of collisions (just for
benchmarking purposes) and a load factor. *The load factor* measures how
full the hash map is. We don’t want to be fuller than the 75%. After
that we are going to do something called *rehash*.

=== Inserting elements in a HashMap

To insert values into a HashMap we first convert the *key* into *an
array index* using the hashFunction. Each bucket of the array has linked
list to hold the values.

There are multiple scenarios for inserting key/values in a HashMap:

1.  Key doesn’t exist yet, we will add the new key/value.
2.  Key already exists, we will update the value and we are done.
3.  Key doesn’t exist, but the bucket already has other data, this is a
collision. Using the linked list, we would push another element to it.

In code it looks like this:

set(key, value) \{

const index = this.hashFunction(key);

this.buckets[index] = this.buckets[index] || new LinkedList();

const bucket = this.buckets[index];

const exists = this.getEntry(key, (entry) => \{

entry.value = value; // update value if key already exists

});

if (!exists) \{ // add key/value if it doesn't find the key

bucket.push(\{ key, value });

this.size += 1;

if (bucket.size > 1) \{ this.collisions += 1; }

if (this.isBeyondloadFactor()) \{ this.rehash(); }

}

return this;

}

Notice, that we are using a function called getEntry to check if the key
already exists. We are going to implement that function next.

=== Rehashing the HashMap

The idea of rehashing is to double the size when the map is getting full
so the collisions are minimized. When we double the size, we try to find
the next prime. We explained that keeping the bucket size a prime number
is beneficial for minimizing collisions.

rehash(newBucketSize = this.buckets.length * 2) \{

const newCapacity = nextPrime(newBucketSize);

const newMap = new HashMap(newCapacity);

for (const key of this.keys()) \{

newMap.set(key, this.get(key));

}

this.reset(newMap);

}

The algorithms for finding next prime is implemented
https://github.com/amejiarosario/algorithms.js/blob/master/src/data-structures/hash-maps/primes.js[here]
and you can find the full HashMap implementation on this file:
https://github.com/amejiarosario/algorithms.js/blob/master/src/data-structures/hash-maps/hashmap.js

=== Getting values out of a HashMap

For getting values out of the Map, we do something similar to inserting.
We convert the key into an index using the hash function.

getEntry(key, callback = () => \{}) \{

const index = this.hashFunction(key);

const bucket = this.buckets[index] || new LinkedList();

return bucket.find((\{ value: entry }) => \{

if (key === entry.key) \{

callback(entry);

return entry;

}

return undefined;

});

}

Later, we use the
https://github.com/amejiarosario/algorithms.js/blob/master/src/data-structures/linked-lists/linked-list.js[find
method] of the linked list to get the node with the matching key. With
getEntry, we can also define get and has method.

get(key) \{

const entry = this.getEntry(key);

return entry && entry.value;

}

has(key) \{

const entry = this.getEntry(key);

return entry !== undefined;

}

For has we only care if the defined or not, while that for get we want
to return the value or undefined if it doesn’t exist.

=== Deleting from a HashMap

Removing items from a HashMap not too different from what we did before:

delete(key) \{

const index = this.hashFunction(key);

const bucket = this.buckets[index];

if (!bucket || bucket.size === 0) \{ return false; }

return !!bucket.remove((node) => \{

if (key === node.value.key) \{

this.size -= 1;

return true;

}

return undefined;

});

}

If the bucket doesn’t exist or is empty we are done. If the value exists
we use the
https://github.com/amejiarosario/algorithms.js/blob/master/src/data-structures/linked-lists/linked-list.js[remove
method] from the linked list.

== HashMap time complexity

Hash Map it’s very optimal for searching values by key *O(1)**. However,
searching values directly is not any better than an array since we have
to visit every value *O(n)*.

[cols=",,,,,",options="header",]
|================================================================
|Data Structure |Searching by |Insert |Delete |Space Complexity |
| |_Index/Key_ |_Value_ | | |
|Hash Map (naïve) |*O(n)* |*O(n)* |*O(n)* |*O(n)* |*O(n)*
|Hash Map (optimized) |*O(1)** |*O(n)* |*O(1)** |*O(1)** |*O(n)*
|================================================================

* = Amortized time. E.g. rehashing might affect run time

As you can notice we have amortized times, since in the unfortunate case
of a rehash, it will take O(n) while it resizes. After that it will be
on average *O(1)*.

The full HashMap implementation with comments can be found on:
https://github.com/amejiarosario/algorithms.js/blob/master/src/data-structures/hash-maps/hashmap.js

== Implementing a TreeMap

Implementing a Map with a tree, TreeMap, has a couple of advantages over
a HashMap:

* Keys are always sorted.
* Statistical data can be easily obtained like median, highest, lowest
key.
* Collisions are not a concern so in the worst case is still *O(log n)*.
* Trees are more space efficient and doesn’t need to allocate memory
beforehand (e.g. HashMap’s initial capacity) nor you have to rehash when
is getting full.

Ok, now that you know the advantages, let’s implement it! For a full
comparison read the link:#hashmap-vs-treemap[HashMap vs TreeMap] section
again.

Let’s get started with the basic functions. They have the same interface
as the HashMap (but obviously the implementation is different).

class TreeMap \{

constructor()\{}

set(key, value) \{}

get(key) \{}

has(key) \{}

delete(key) \{}

}

=== Inserting values into a TreeMap

For inserting a value on a TreeMap, we first need to inialize the tree:

class TreeMap \{

constructor() \{

this.tree = new Tree();

}

}

The tree, can be an instance of any Binary Search Tree that we
implemented so far. However, for better performance it should be a
self-balanced tree like a
https://github.com/amejiarosario/algorithms.js/blob/master/src/data-structures/trees/red-black-tree.js[Red-Black
Tree] or
https://github.com/amejiarosario/algorithms.js/blob/master/src/data-structures/trees/avl-tree.js[AVL
Tree].

set(key, value) \{

return this.tree.add(key).data(value);

}

get size() \{

return this.tree.size;

}

Adding values is very easy (once we have the implementation).

=== Getting values out of a TreeMap

We search by key which takes *O(log n)* on balanced trees.

get(key) \{

const node = this.tree.get(key) || undefined;

return node && node.getData();

}

has(key) \{

return !!this.get(key);

}

One side effect of storing keys in a tree is that they can be retrieve
in order.

* [Symbol.iterator]() \{

yield* this.tree.inOrderTraversal();

}

* keys() \{

for (const node of this) \{

yield node.value;

}

}

We can use the *in-order traversal* for a BST.

=== Deleting values from a TreeMap

Removing elements from TreeMap is simple.

delete(key) \{

return this.tree.remove(key);

}

The BST implementation does all the heavy lifting.

That’s basically it! To see the full file in context, click here:
https://github.com/amejiarosario/algorithms.js/blob/master/src/data-structures/maps/tree-maps/tree-map.js[https://github.com/amejiarosario/algorithms.js/blob/master/src/data-structures/maps/tree-maps/tree-map.js]

== TreeMap Time complexity vs HashMap

As we discussed so far, there are trade-off between the implementations

[cols=",,,,,",options="header",]
|=======================================================================
|Data Structure |Searching by |Insert |Delete |Space Complexity |
| |_Index/Key_ |_Value_ | | |

|Hash Map (Imperfect) |*O(n)* |*O(n)* |*O(n)* |*O(n)* |*O(n)*

|Hash Map (optimized) |*O(1)** |*O(n)* |*O(1)** |*O(1)** |*O(n)*

|Tree Map (Red-Black Tree) |*O(log n)* |*O(n)* |*O(log n)* |*O(log n)*
|*O(n)*
|=======================================================================

* = Amortized time. E.g. When rehashing is due it would take *O(n)*.

= Set

A set is a data structure where duplicated are not allowed. JavaScript
has already a built-in Set data structure. Take a look at the following
example:

const set = new Set();

set.add(1); //↪️ Set [ 1 ]

set.add(1); //↪️ Set [ 1 ]

set.add(2); //↪️ Set [ 1, 2 ]

set.add(3); //↪️ Set [ 1, 2, 3 ]

set.has(1); //↪️ true

set.delete(1); //↪️ removes 1 from the set

set.has(1); //↪️ false, 1 has been removed

set.size; //↪️ 2, we just removed one value

console.log(set); //↪️ Set(2) \{2, 3}

As you can see, even if we insert the same value multiple times, it only
gets added once.

Can you think in a way how to implement it?

… A hint: it should perform all operations in *O(1)** or at most *O(log
n)*

If we use a Map’s keys we can accomplish this, right? They cannot be
duplicates and we could just ignore the value part.

== HashSet vs TreeSet

As we saw earlier, we have to ways of implanting maps using a *balanced
BST* and using a *hash function*. If we use them to implement a *Set*
then we would have a *HashSet* and *TreeSet* respectively.

* *TreeSet, would return the values sorted in ascending order.*
* *HashSet, would return the values in insertion order.*
* *Operations on a HashSet would take on average O(1) and in the worst
case (rehash is due), it would take O(n).*
* *Operation on a TreeSet is always O(log n)*.

Let’s implement both!

== Implementing a TreeSet

We are to use a balanced BST (Red-Black Tree) to implement TreeSet.
Other self-balanced tree implementations will do.

const Tree = require('../trees/red-black-tree');

/**

* TreeSet implements a Set (collection of unique values)

* using a balanced binary search tree to guarantee a O(log n) in all
operations.

*/

class TreeSet \{

/**

* Initialize tree and accept initial values.

* @param \{array} iterable initial values (duplicates will be added
once)

*/

constructor(iterable = []) \{

this.tree = new Tree();

Array.from(iterable).forEach(value => this.add(value));

}

/**

* Size of the set

*/

get size() \{

return this.tree.size;

}

We also want to convert arrays with duplicates into a set so we have
only unique values. We can do that by passing them in the constructor.
E.g.:

set = new TreeSet([1, 2, 3, 1]);

expect(set.size).toBe(3);

expect(Array.from(set.keys())).toEqual([1, 2, 3]);

Ok, now let’s implement the add method.

=== Adding elements to a TreeSet

For adding values to the set, we just pass it to the Tree add method.

/**

* Add a new value (duplicates will be added only once)

* Runtime: O(log n)

* @param \{any} value

*/

add(value) \{

if (!this.has(value)) \{

this.tree.add(value);

}

}

If you remember, our BST implementation allows duplicated values. It has
a multiplicity tally to keep track of duplicates. However, we don’t want
that in a set, we want each value only once. For that we check if the
value is already in the tree. Don’t worry for adding extra lookups. The
has is also very performant *O(log n)*. Let’s implement it!

=== Searching for values in a TreeSet

Again, we rely on the Tree implementation to do the heavy lifting:

/**

* Check if value is already on the set

* Runtime: O(log n)

* @param \{any} value

* @returns \{boolean} true if exists or false otherwise

*/

has(value) \{

return !!this.tree.get(value);

}

The get method retrieve the data, but we only need a Boolean. We are
using the !! to convert the result to a Boolean.

=== Converting TreeSet to Array

A common use case to consume the data from a Set is to covert it to an
array or use in an iterator (for loops, forEach, …). Let’s provide the
method for that:

/**

* Default iterator for this set

* @returns \{iterator} values in ascending order

*/

* [Symbol.iterator]() \{

for (const node of this.tree.inOrderTraversal()) \{

yield node.value;

}

}

We are using the inOrderTraversal method of the BST to go each key in an
ascending order.

JavaScript Built-in Symbol iterator

The Symbol.iterator built-in symbol specifies the default iterator for
an object. Used by for...of, Array.from and others.

Now we can convert from set to array and vice versa easily. For
instance:

const array = [1, 1, 2, 3, 5];

// array to set

const set = new TreeSet(array);

// set to array

Array.from(set); //↪️ (4) [1, 2, 3, 5]

No more duplicates in our array!

=== Deleting elements from a TreeSet

We delete the elements from the TreeSet using the remove method of the
BST.

/**

* Delete a value from the set

* Runtime: O(log n)

* @param \{any} value

*/

delete(value) \{

return this.tree.remove(value);

}

Voilà! That’s it!

Check out the full implementation on:

Let’s now, implement a HashSet.

== Implementing a HashSet

The *HashSet* is the set implementation using a HashMap as its
underlying data structure.

The HashSet interface will be exactly the same as the built-in Set or
our TreeSet.

const HashMap = require('../maps/hash-maps/hashmap');

/**

* Set implemented with our HashMap

* Have an average of O(1) time on all operations

*/

class HashMapSet \{

/**

* Initialize (Hash)map for the set

*

* @param \{Array} iterable If passed, all iterable elements will be
added to the new Set

*/

constructor(iterable = []) \{

this.hashMap = new HashMap();

Array.from(iterable).forEach(element => this.add(element));

}

}

This constructor is useful for converting an array to set and
initializing the HashMap.

=== Inserting values to a HashSet

To insert items in a HashSet we use the set function of the HashMap:

/**

* Add a new value (duplicates will be added only once)

* @param \{any} value

*/

add(key) \{

this.hashMap.set(key);

}

HashMap stores key/value pairs, but for this we only need the key and we
ignore the value.

=== Finding values in a HashSet

To find values in a HashMap we use the has method.

/**

* check if value is already on the set

* @param \{any} value

*/

has(key) \{

return this.hashMap.has(key);

}

The HashMap will convert the key into an array index using a hash
function. If there’s something in the array index bucket, it will return
true, if it’s empty it will be false.

=== Deleting values from a HashSet

For deleting a value from a hashSet we use the HashMap’s delete method:

/**

* Delete a value from the set

* Avg. Runtime: O(1)

* @param \{any} value

*/

delete(value) \{

return this.hashMap.delete(value);

}

This method has an Average runtime of *O(1)*.

== HashSet vs HashMap Time Complexity

We can say that HashMap in on average more performant, however if a
rehash happens it will take *O(n)* instead of *O(1)*. While TreeSet is
always *O(log n)*.

[cols=",,,,,",options="header",]
|================================================================
|Data Structure |Searching by |Insert |Delete |Space Complexity |
| |_Index/Key_ |_Value_ | | |
|HashSet |- |*O(1)** |*O(1)** |*O(1)** |*O(n)*
|HashMap |- |*O(log n)* |*O(log n)* |*O(log n)* |*O(n)*
|================================================================

* = Amortized time. E.g. rehashing might affect run time

To recap, HashSet and TreeSet will keep data without duplicates. The
difference besides runtime is that:

* HashSet keeps data in insertion order
* TreeSet keeps data sorted in ascending order.

= Graph Data Structure

Graphs is one of my favorite data structures. They have a lot of cool
applications and are used in more places than you can imagine. First,
let’s start with the basics.

A graph is a non-linear data structure where a node can have zero or
more linked nodes.

You can think of graph like an extension of a Linked List. Instead of
having only a next or previous linked node, you can have as many as you
want. Actually, you can an array of linked nodes.

/**

* Graph node/vertex that hold adjacencies nodes

*/

class Node \{

constructor(value) \{

this.value = value;

this.adjacents = []; // adjacency list

}

}

As you can see, it’s pretty similar to the Linked List node indeed. The
only difference is that uses an *array* of the linked nodes instead.

Other difference between a linked list and graph is that linked list
have a start/first/root node, while the graph doesn’t. You can start
traversing a graph from anywhere and there might be circular references
as well. Let’s study this graph properties!

== Graph Properties

The connection between two nodes is called *edge*. Also, nodes might be
called *vertex*.

image:extracted-media/media/image42.png[image,width=305,height=233]

Figure 26 - Graph is composed of vertices/nodes and edges

=== Directed Graph vs Undirected

A graph can *directed* and *undirected*.

image:extracted-media/media/image43.jpg[image,width=469,height=192]

Figure 27 - Graph: directed vs undirected

A *directed graph (digraph)* has edges that are *one-way street*. E.g.
On the directed example, you can only go from green node to orange and
not the other way around. When one node has an edge to itself is called
a *self-loop*.

An *undirected graph* has edges that are *two-way street*. E.g. On the
undirected example, you can traverse from the green node to the orange
and vice versa.

=== Graph Cycles

A graph can have *cycles* or not.

image:extracted-media/media/image44.jpg[image,width=444,height=194]

Figure 28 - Cyclic vs Acyclic Graphs.

A *cyclic graph* is the one that you can pass through a node more than.
E.g. On the cyclic illustration, if you start in the green node, then go
the orange and purple, finally, you could come back to green again.
Thus, it has a *cycle*.

All *undirected* graphs are cyclic but not all *directed* graphs are
cyclic.

An acyclic graph is the one that you can’t pass through a node more than
once. E.g. in the acyclic illustration, can you to find a path where you
can pass through the same vertex more than one?

*Directed Acyclic Graphs (DAG)* are also known as a *Tree* when each
node has only *one parent*.

=== Connected vs Disconnected vs Complete Graphs

image:extracted-media/media/image45.emf[image,width=528,height=176]

Figure 29 - Different kinds of graphs: disconnected, connected, and
complete.

A *disconnected graph* is one that have one or more subgraph. In other
words, a graph is *disconnected* if there are two nodes that doesn’t
have a path between them.

A *connected graph* is the opposite to disconnected, there’s a path
between every node.

A *complete graph* is where every node is adjacent to all the other
nodes in the graph. E.g. If there are 7 nodes, every node has 6 edges.

=== Weighted Graphs

Weighted graphs have labels in the edges. The label is called *weight*
or *cost*. The weight can represent many things like distance, travel
time, or anything else.

image:extracted-media/media/image46.png[image,width=528,height=337]

Figure 30 - Weighted Graph representing USA airports distance in miles.

For instance, a weighted graph can have the distance between nodes. So,
algorithms can use the weight and optimize the path between them.

== Exciting Graph applications in real-world

Now that we know what graphs are and some of their properties let’s
discuss about some real-life usages of graphs.

Graphs become a metaphor where nodes and edges model something from our
physical world. Just to name a few:

* Optimizing Plane traveling

* Nodes = Airport
* Edges = Direct flights between two airports
* Weight = miles between airports | cost | time

* GPS Navigation System

* Node = road intersection
* Edge = road
* Weight = time between intersections

* Network routing

* Node = server
* Edge = data link
* Weight = connection speed

There are endless applications for graphs in electronics, social
networks, recommendation systems and many more. That’s cool and all, but
how do we represent graphs in code? Let’s see that in the next section.

== Representing Graphs

There are two main ways to graphs one is:

* Adjacency Matrix
* Adjacency List

=== Adjacency Matrix

Representing graphs as adjacency matrix is done using a two-dimensional
array. For instance, let’s say we have the following graph:

image:extracted-media/media/image47.png[image,width=438,height=253]

Figure 31 - Graph and its adjacency matrix.

The size of the matrix is given by the number of vertices |V|, in the
example we have 5 vertices so we have a 5x5 matrix.

To fill up the matrix, we go row by row. Mark with 1 (or any other
weight) when you find an edge. E.g.

* Row 0: It has a self-loop, so it has a 1 in the intersection of 0,0.
The node 0 also has an edge to 1 and 4 so we mark it.
* Row 1: The node 1 has one edge to 3 so we mark it.
* Row 2: Node 2 goes to Node 4, so we mark the insertion with 1.
* And so on…

The example graph above is a directed graph (digraph). In case of
undirected graph, the matrix would be symmetrical by the diagonal.

If we represent the example graph in code, it would be something like
this:

_const_ digraph = [

[1, 1, 0, 0, 1],

[0, 0, 0, 1, 0],

[0, 0, 0, 0, 1],

[0, 0, 1, 0, 0],

[0, 1, 0, 0, 0],

];

It would be very easy to tell if two nodes are connected. Let’s query if
node 2 is connected to 3:

digraph[2][3]; _//=> 0_

digraph[3][2]; _//=> 1_

As you can see we don’t have a link from node 2 to 3, but we do in the
opposite direction. Querying arrays is constant time *O(1)*, so no bad
at all.

The issue with the adjacency matrix is the space it takes. Let’s say you
want to represent the entire Facebook network on a digraph. You would
have a huge matrix of 1.2 billion x 1.2 billion. The worst part is that
most of it would be empty (zeros) since people are connected to at most
few thousands.

When the graph has few connections compared to the number of nodes we
say that we have a *sparse graph*. On the opposite, if we have almost
complete graphs we say we have a *dense graph*.

The space complexity of adjacency matrix is *O(|V|^2^)*, where |V| is
the number of vertices/nodes.

=== Adjacency List

Another way to represent a graph is using an adjacency list. This time
instead of using an array (matrix) we use a list.

image:extracted-media/media/image48.png[image,width=528,height=237]

Figure 32 – Graph represented as an Adjacency List.

Body

== Adding a vertex

Body text

== Adding an edge

Body text

== Querying Adjacency

Body text

== Deleting a vertex

Body text

== Deleting an edge

Body text

== Graph Complexity

Graph search has its own chapter

= Summary

Body text

4

[[_Toc525822218]]Learning Fast Sorting Algorithms

Introduction.

* _______
Topic 1
_______
* _______
Topic 2
_______
* _______
Topic 3
_______

= Avoiding Slow Sorting Algorithms

Iterate and expand on the sub-topic.

== Selection Sort

Body text

== Bubble Sort

Body text

== Insertion Sort

Body text

= Understanding Efficient Sorting Algorithms

Iterate and expand on the sub-topic.
https://en.wikipedia.org/wiki/Sorting_algorithm[https://en.wikipedia.org/wiki/Sorting_algorithm#Comparison_of_algorithms]

== Merge Sort

Stable but uses additional memory, Block merge sort uses constant memory
https://en.wikipedia.org/wiki/Block_sort

The entire input must be iterated through, and this must occur O(log(n))
times (the input can only be halved O(log(n)) times). n items iterated
log(n) times gives O(n log(n)).

== Quick Sort

Body text

A binary search tree is a dynamic version of what happens during
quicksort.

== Tim Sort

Stable but use additional memory

== Heapsort

Body text

== Radix Sort

t's been proven that no comparison sort can operate faster than this.
Only sorts that rely on a special property of the input such as radix
sort can beat this complexity. The constant factors of mergesort are
typically not that great though so algorithms with worse complexity can
often take less time.
https://softwareengineering.stackexchange.com/a/297161/106607

A trie is a dynamic version of what happens during radix sort.

= Summary

Body text

5

[[_Toc525822222]]Searching Efficiently

Introduction.

* _______
Topic 1
_______
* _______
Topic 2
_______
* _______
Topic 3
_______

= Linear Search

Iterate and expand on the sub-topic.

== Linear Search

Body text

== Binary Search

Body text

== Sub-topic

Body text

= Searching in a Graph

Iterate and expand on the sub-topic.

== Depth First Search (DFS)

Body text

== Breadth First Search (BFS)

Body text

== Sub-topic

Body text

= Shortest Path with Dijkstra

Iterate and expand on the sub-topic.

== Sub-topic

Body text

== Sub-topic

Body text

== Sub-topic

Body text

= Summary

Body text

5

[[_Toc525822227]]Balancing Binary Search Trees for Max Performance

Introduction.

* _______
Topic 1
_______
* _______
Topic 2
_______
* _______
Topic 3
_______

= Tree Rotations

Iterate and expand on the sub-topic.

== Left Rotation

Body text

== Right Rotation

Body text

== Left-Right Rotation

Body text

== Right-Left Rotation

Body text

= AVL Tree

Iterate and expand on the sub-topic.

== Insertion

Body text

== Search by Value

Body text

== Deletion

Body text

= Summary

Body text

0

[[_Toc525822231]]Algorithmic Thinking

Introduction. Firstly, address your headings. Next introduce _yourself_
to the chapter. Start with the topic. What is it. Tell them why it’s
useful. Now explain your chapter structure. What key milestones will hit
throughout the chapter.

Reiterate the chapter structure with bullet points:

* _______
Topic 1
_______
* _______
Topic 2
_______
* _______
Topic 3
_______

= Algorithmic Paradigms

Write your heading. Your headings should generally always try to tell
the reader what they will be _doing_ with the section. A useful device
are “gerund” words. These are –ing words, like “Implementing”,
“Building, “Creating”, “Programming”, “Testing.

Iterate and expand on the sub-topic. Explain what the sub-topic is.
Where does it fit in to the wider topic? Explain the key steps/subtopics
the reader will perform.

Towards the end, outline any prerequisites the reader will need – will
they need anything new installed? Will they want any specific files or
programmes open?

== Brute Force

Body text. Now outline the key steps needed to perform the topic.

Linear search

== Greedy

Body text,

A Dijkstra Algorithm - finding shortest path to all graph vertices

== Divide and Conquer

Binary Search,
https://github.com/trekhleb/javascript-algorithms#algorithms-by-paradigm

B Merge Sort

B Quicksort

B Tree Depth-First Search (DFS)

B Graph Depth-First Search (DFS)

== Dynamic Programming

Binary Search,

= Topic

Iterate and expand on the sub-topic.

== Sub-topic

Body text

== Sub-topic

Body text

== Sub-topic

Body text

= Topic

Iterate and expand on the sub-topic.

== Sub-topic

Body text

== Sub-topic

Body text

== Sub-topic

Body text

= Summary

Body text

0

[[_Toc525822236]]Stepping up your game with Advanced Data Structures

Introduction.

* _______
Topic 1
_______
* _______
Topic 2
_______
* _______
Topic 3
_______

= Heap

Iterate and expand on the sub-topic.

== Insert

Body text

== Heapify

Body text

== Find max/min

Body text

== Extract max/min

Body text

== Increase Key

Body text

== Delete

Body text

== Merge

Body text

= Tries

Iterate and expand on the sub-topic.
https://github.com/trekhleb/javascript-algorithms/tree/master/src/data-structures/trie

Why Trie? :-

1.  With Trie, we can insert and find strings in O(L) time where L
represent the length of a single word. This is obviously faster that
BST. This is also faster than Hashing because of the ways it is
implemented. We do not need to compute any hash function. No collision
handling is required (like we do in open addressing and separate
chaining)
2.  Another advantage of Trie is, we can easily print all words in
alphabetical order which is not easily possible with hashing.
3.  We can efficiently do prefix search (or auto-complete) with Trie.

Issues with Trie :-

The main disadvantage of tries is that they need lot of memory for
storing the strings. For each node we have too many node pointers(equal
to number of characters of the alphabet), If space is concern, then
Ternary Search Tree can be preferred for dictionary implementations. In
Ternary Search Tree, time complexity of search operation is O(h) where h
is height of the tree. Ternary Search Trees also supports other
operations supported by Trie like prefix search, alphabetical order
printing and nearest neighbor search.

https://thenextcode.wordpress.com/2015/04/12/trie-vs-bst-vs-hashtable/

https://en.wikipedia.org/wiki/Deterministic_acyclic_finite_state_automaton

http://jayant7k.blogspot.com/2011/06/data-structures-trie.html

The final conclusion is regarding tries data structure is that they are
faster but require huge memory for storing the strings.

Binary Tree, BST, Heaps, Tries, …

Body text
https://en.wikipedia.org/wiki/Heap_(data_structure)[https://en.wikipedia.org/wiki/Heap_(data_structure)#Comparison_of_theoretic_bounds_for_variants]

== Applications

Body text

== Insert word

Body text

== Suggesting next characters

Body text

== Delete Word

Body text

Summary

Body text

Code

_const_ Node = require('./node');

_/**_

_* Doubly linked list that keeps track of_

_* the last and first element_

_*/_

_class_ LinkedList \{

_constructor_() \{

this.first = null; // head/root element

_this_.last = null; _// last element of the list_

_this_.size = 0; _// total number of elements in the list_

}

}

===== Testing.ts

// code

Code end

High 0

Highend

$ curl –-path-as-is http://localhost:3000/../test.txt

Big O Cheatsheet

[cols=",,,,,,,,,",options="header",]
|=======================================================================
|Data Structure |Searching by |Inserting at the |Deleting from the
|Space Complexity | | | | |
| |_Index/Key_ |_Value_ |_start_ |_middle_ |_end_ |_start_ |_middle_
|_end_ |

|Array |*O(1)* |*O(n)* |*O(n)* |*O(n)* |*O(1)* |*O(n)* |*O(n)* |*O(1)*
|*O(n)*

|Linked List (singly) |*O(n)* |*O(n)* |*O(1)* |*O(n)* |*O(1)* |*O(1)*
|*O(n)* |*O(n)* |*O(n)*

|Linked List (doubly) |*O(n)* |*O(n)* |*O(1)* |*O(n)* |*O(1)* |*O(1)*
|*O(n)* |*O(1)* |*O(n)*

|Stack |- |- |- |- |*O(1)* |- |- |*O(1)* |*O(n)*

|Queue (w/array) |- |- |*O(n)* |- |- |- |- |*O(1)* |*O(n)*

|Queue (w/list) |- |- |*O(1)* |- |- |- |- |*O(1)* |*O(n)*
|=======================================================================

[cols=",,,,,",options="header",]
|=======================================================================
|Data Structure |Searching by |Insert |Delete |Space Complexity |
| |_Index/Key_ |_Value_ | | |

|Binary Search Tree (unbalanced) |- |*O(n)* |*O(n)* |*O(n)* |*O(n)*

|Binary Search Tree (balanced: AVL tree) |- |*O(log n)* |*O(log n)*
|*O(log n)* |*O(n)*

|Hash Map (Imperfect) |*O(n)* |*O(n)* |*O(n)* |*O(n)* |*O(n)*

|Hash Map (optimized) |*O(1)** |*O(n)* |*O(1)** |*O(1)** |*O(n)*

|Tree Map |*O(log n)* |*O(n)* |*O(log n)* |*O(log n)* |*O(n)*

|Set (using Hash Map) |- |*O(1)** |*O(1)** |*O(1)** |*O(n)*

|Set (using Tree Map) |- |*O(log n)* |*O(log n)* |*O(log n)* |*O(n)*
|=======================================================================

* = Amortized time. E.g. rehashing might affect run time

image:extracted-media/media/image49.jpeg[image,width=528,height=186]

Implementing an LRU Cache with HashMap

Discards the least recently used items first. 

https://leetcode.com/problems/lru-cache/description/

TODO: Compare content with:

* https://adrianmejia.com/blog/2018/04/28/data-structures-time-complexity-for-beginners-arrays-hashmaps-linked-lists-stacks-queues-tutorial/[https://adrianmejia.com/blog/2018/04/28/data-structures-time-complexity-for-beginners-arrays-hashmaps-linked-lists-stacks-queues-tutorial/#Stacks]
* https://leetcode.com/explore/learn/
* https://github.com/trekhleb/javascript-algorithms
* Compare with: Data Structures and Algorithms.pdf by Lydia Hallie
* Cracking code interviews
* Grokking Algorithms
* CS Distilled
* Create poster like: http://bigocheatsheet.com/, http://bigoref.com/,
* Princeton
** https://introcs.cs.princeton.edu/java/11cheatsheet/
