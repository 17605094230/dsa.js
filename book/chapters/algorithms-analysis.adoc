= Learning Algorithms Analysis

Chances are you are reading this book because you want to write better and faster code.
How can you do that? Can you time how long it takes to run a program? Of course, you can!
[big]#‚è±#
However, if you run the same program on a smart watch, cellphone or desktop computer it will give you different times.

image:image3.png[image,width=528,height=137]

Wouldn't it be great if we can compare algorithms regardless of the hardware where we run them?
That's what *time complexity* is for!
But why stop with the running time?
We could also compare the memory "used" by different algorithms and we called that *space complexity*.

.In this chapter you will learn:
-	What‚Äôs the best way to measure your code performance.
-	Learn how to use Big O notation to compare algorithms.
-	How to use algorithms analysis to improve your programs speed.

== Comparing Algorithms

Before going deeper, into space and time complexity, let's define what an algorithm is.

Algorithms (as you might know) are steps of how to do some task. When you cook, you follow a recipe (or an algorithm) to prepare a dish. If you play a game, you are devising strategies (or an algorithm) to help you win. Likewise, algorithms in computers are a set of instructions used to solve a problem.

IMPORTANT: Algorithms are instructions to perform a task

There are ‚Äúgood‚Äù algorithms and ‚Äúbad‚Äù algorithms. The good ones are fast; the bad ones are slow. Slow algorithms cost more money and make some calculations impossible in our lifespan!

Just to give you a clearer picture how different algorithms perform as the input size grows.

.Relationship between algorithm input size and time taken to complete
[cols=",,,,,",options="header",]
|=============================================================================================
|Input size -> |10 |100 |10k |100k |1M
|Finding if a number is odd |< 1 sec. |< 1 sec. |< 1 sec. |< 1 sec. |< 1 sec.
|Sorting elements in array with merge sort |< 1 sec. |< 1 sec. |< 1 sec. |few sec. |20 sec.
|Sorting elements in array with Bubble Sort |< 1 sec. |< 1 sec. |2 minutes |3 hours |12 days
|Finding all subsets of a given set |< 1 sec. |40,170 trillion years |> centillion years |‚àû |‚àû
|Find all permutations of a string |4 sec. |> vigintillion years |> centillion years |‚àû |‚àû
|=============================================================================================

You can really notice the difference between a good algorithm and bad with the sorting array elements examples: merge-sort vs bubble sort.
Organizing 1 million elements with merge sort takes 20 seconds while bubble sort takes 12 days, ouch!
The amazing thing is that both programs are measured on the same hardware with exactly the same data!

After completing this book, you are going to *think differently*.
You will be able to scale your programs while you are designing them.
Find bottlenecks of existing software and have an "algorithmic toolbox" to switch algorithms and make them faster without having to upgrade hardware. [big]#üí∏#

== Increasing your code performance

The first step to improve your code performance is to measure it. As somebody said:

[quote, H. J. Harrington]
Measurement is the first step that leads to control and eventually to improvement. If you can‚Äôt measure something, you can‚Äôt understand it. If you can‚Äôt understand it, you can‚Äôt control it. If you can‚Äôt control it, you can‚Äôt improve it.

In this section we are going to learn the basics to measuring our current code performance and compare it with others.

=== Calculating Space and Time Complexity

Time complexity, in computer science, is a function that describes the amount of operations a program will execute given the size of the input `n`. The same applies for space complexity but instead of the amount of operations executed it will be the amount of memory used additional to the input.

How do get a function that give us the amount of operations that will executed? Well, we count line by line and mind code inside loops. For instance, we have a function to find the minimum value on an array called `getMin`.

.Translating lines of code to approximate number of operations
image:image4.png[Operations per line]

Assuming that each line of code is an operation, we get the following that the number of operations given the input size `n` is:

_3n + 3_

That means that if give an array of 3 elements e.g. `getMin([3, 2, 9])`, then it will execute around _3(3)+3 = 12_ operations. Of course, this is not exact. Line 12 is only executed if the condition on line 11 is met. As you might learn in the next section, we want to get the big picture and get rid of smaller terms in order to compare algorithms easier.

Calculating the *space complexity* is very similar but, instead of operations, we keep track of the ‚Äúvariables‚Äù and memory used. In the `getMin` example, we just create a single variable called `min`. So, the space complexity is 1. If we had to copy values to another array then the space complexity would be `n`.

=== Simplifying Complexity with Asymptotic Analysis

Asymptotic analysis is the of functions when their inputs approaches infinity.

In the previous example we analyzed `getMin` with an array of size 3, what happen size is 10 or 10k or a million?

.Operations performed by an algorithm with a time complexity of 3n+3
[cols=",,",options="header",]
|===========================
|n (size) |Operations |total
|10 |3(10) + 3 |33
|10k |3(10k)+3 |30,003
|1M |3(1M)+3 |3,000,003
|===========================

As the input size n grows bigger and bigger then the expression _3n+3_ could be represented as _3n_ or even _n_. This might look like a stretch at first, but you will see that what matters the most is the order of the function rather than lesser terms and constants. Actually, there‚Äôs a notation called *Big O*, where O refers to the *order of the function*.

If you have a program which run time is like

_7n^3^ + 3n^2^ + 5_

You can safely say that its run time is _n^3^_ since the others term will become less and less significant as the inputs grows bigger.

=== What is Big O Notation anyways?

Big O notation, only cares about the ‚Äúbiggest‚Äù terms in the time/space complexity. So, it combines what we learn about time and space complexity, asymptotic analysis and adds worst-case scenario.

.All algorithms have 3 scenarios:
* Best-case scenario: the most favorable input where the program will take the least amount of operations to complete. E.g. array already sorted for a sorting algorithm.
* Average-case scenario: the most common the input comes. E.g. array items in random order for a sorting algorithm.
* Worst-case scenario: the inputs are arranged in such a way that cause the program to take the longest possible to complete the task. E.g. array items in reversed order for a sorting algorithm.

To sum up:

IMPORTANT: Big O only cares about the highest order of the run time function and the worst-case scenario.
There are many common notations like polynomial, _O(n^2^)_ like we saw in the getMin example; constant O(1) and many more that we are going to explore in the next chapter.

Again, time complexity is not a direct measure of how long a program takes to execute but rather how many operations it executes in function of the input. However, there‚Äôs a relationship between time and operations executed. This changes from hardware to hardware but it gives you an idea.

Readers might not know what this O(n!) means‚Ä¶

.How long an algorithm takes to run based on their time complexity and input size
[cols=",,,,,,",options="header",]
|===============================================================
|Input Size |O(1) |O(n) |O(n log n) |O(n^2^) |O(2^n^) |O(n!)
|1 |< 1 sec. |< 1 sec. |< 1 sec. |< 1 sec. |< 1 sec. |< 1 sec.
|10 |< 1 sec. |< 1 sec. |< 1 sec. |< 1 sec. |< 1 sec. |4 seconds
|10k |< 1 sec. |< 1 sec. |< 1 sec. |2 minutes |‚àû |‚àû
|100k |< 1 sec. |< 1 sec. |1 second |3 hours |‚àû |‚àû
|1M |< 1 sec. |1 second |20 seconds |12 days |‚àû |‚àû
|===============================================================

This just an illustration since in a different hardware the times will be slightly different.

NOTE: These times are under the assumption of running on 1 GHz CPU and that it can execute on average one instruction in 1 nanosecond (usually takes more time). Also, bear in mind that each line might be translated into dozens of CPU instructions depending on the programming language. Regardless, bad algorithms still perform badly even in a super computer.

== Summary

In this chapter we learned how you can measure your algorithm performance using time complexity. Rather than timing how long you program take to run you can approximate the number of operations it will perform based on the input size.

We went thought the process of deducting the time complexity from a simple algorithm. We learned about time and space complexity and how they can be translated to Big O notation. Big O refers to the order of the function.

In the next section, we are going to provide examples of each of the most common time complexities!
